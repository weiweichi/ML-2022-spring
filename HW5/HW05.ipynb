{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFEKWoh3p1Mv"
      },
      "source": [
        "# Homework Description\n",
        "- English to Chinese (Traditional) Translation\n",
        "  - Input: an English sentence         (e.g.\t\ttom is a student .)\n",
        "  - Output: the Chinese translation  (e.g. \t\t湯姆 是 個 學生 。)\n",
        "\n",
        "- TODO\n",
        "    - Train a simple RNN seq2seq to acheive translation\n",
        "    - Switch to transformer model to boost performance\n",
        "    - Apply Back-translation to furthur boost performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Vf1Q79XPQ3D",
        "outputId": "6bc99387-e873-4bbf-9d81-1706904df370"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Apr  7 02:34:47 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
            "| 41%   57C    P2    65W / 275W |   7268MiB / 11175MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1162      G   /usr/lib/xorg/Xorg                 35MiB |\n",
            "|    0   N/A  N/A      1722      G   /usr/lib/xorg/Xorg                116MiB |\n",
            "|    0   N/A  N/A      1849      G   /usr/bin/gnome-shell                8MiB |\n",
            "|    0   N/A  N/A      2324      G   ...RendererForSitePerProcess       19MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59neB_Sxp5Ub"
      },
      "source": [
        "# Download and import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRlFbfFRpZYT",
        "outputId": "213ea67f-5983-4b20-ca76-f8730309ab2d"
      },
      "outputs": [],
      "source": [
        "# !pip install 'torch>=1.6.0' editdistance matplotlib sacrebleu sacremoses sentencepiece tqdm wandb\n",
        "# !pip install --upgrade jupyter ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSksMTdmp-Wt",
        "outputId": "34655423-fe36-4bbd-fd7a-472ab49d3b6e"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/pytorch/fairseq.git\n",
        "# !cd fairseq && git checkout 9a1c497\n",
        "# !pip install --upgrade ./fairseq/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uRLTiuIuqGNc"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import pdb\n",
        "import pprint\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "import tqdm.auto as tqdm\n",
        "from pathlib import Path\n",
        "from argparse import Namespace\n",
        "from fairseq import utils\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n07Za1XqJzA"
      },
      "source": [
        "# Fix random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xllxxyWxqI7s"
      },
      "outputs": [],
      "source": [
        "seed = 10942178\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  \n",
        "np.random.seed(seed)  \n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5ORDJ-2qdYw"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "## En-Zh Bilingual Parallel Corpus\n",
        "* [TED2020](#reimers-2020-multilingual-sentence-bert)\n",
        "    - Raw: 398,066 (sentences)   \n",
        "    - Processed: 393,980 (sentences)\n",
        "    \n",
        "\n",
        "## Testdata\n",
        "- Size: 4,000 (sentences)\n",
        "- **Chinese translation is undisclosed. The provided (.zh) file is psuedo translation, each line is a '。'**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQw2mY4Dqkzd"
      },
      "source": [
        "## Dataset Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXT42xQtqijD",
        "outputId": "3b09f268-4e1e-4bc9-d42f-e20c68b38017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "raw.en\n",
            "raw.zh\n",
            "test/\n",
            "test/test.zh\n",
            "test/test.en\n"
          ]
        }
      ],
      "source": [
        "data_dir = './DATA/rawdata'\n",
        "dataset_name = 'ted2020'\n",
        "urls = (\n",
        "    \"https://github.com/yuhsinchan/ML2022-HW5Dataset/releases/download/v1.0.2/ted2020.tgz\",\n",
        "    \"https://github.com/yuhsinchan/ML2022-HW5Dataset/releases/download/v1.0.2/test.tgz\",\n",
        ")\n",
        "file_names = (\n",
        "    'ted2020.tgz', # train & dev\n",
        "    'test.tgz', # test\n",
        ")\n",
        "prefix = Path(data_dir).absolute() / dataset_name\n",
        "\n",
        "prefix.mkdir(parents=True, exist_ok=True)\n",
        "for u, f in zip(urls, file_names):\n",
        "    path = prefix/f\n",
        "    if not path.exists():\n",
        "        !wget {u} -O {path}\n",
        "    if path.suffix == \".tgz\":\n",
        "        !tar -xvf {path} -C {prefix}\n",
        "    elif path.suffix == \".zip\":\n",
        "        !unzip -o {path} -d {prefix}\n",
        "!mv {prefix/'raw.en'} {prefix/'train_dev.raw.en'}\n",
        "!mv {prefix/'raw.zh'} {prefix/'train_dev.raw.zh'}\n",
        "!mv {prefix/'test/test.en'} {prefix/'test.raw.en'}\n",
        "!mv {prefix/'test/test.zh'} {prefix/'test.raw.zh'}\n",
        "!rm -rf {prefix/'test'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLkJwNiFrIwZ"
      },
      "source": [
        "## Language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_uJYkCncrKJb"
      },
      "outputs": [],
      "source": [
        "src_lang = 'en'\n",
        "tgt_lang = 'zh'\n",
        "\n",
        "data_prefix = f'{prefix}/train_dev.raw'\n",
        "test_prefix = f'{prefix}/test.raw'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t2CPt1brOT3",
        "outputId": "14cd34e0-c8cb-4653-84c3-29de187d201a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thank you so much, Chris.\n",
            "And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\n",
            "I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.\n",
            "And I say that sincerely, partly because  I need that.\n",
            "Put yourselves in my position.\n",
            "非常謝謝你，克里斯。能有這個機會第二度踏上這個演講台\n",
            "真是一大榮幸。我非常感激。\n",
            "這個研討會給我留下了極為深刻的印象，我想感謝大家 對我之前演講的好評。\n",
            "我是由衷的想這麼說，有部份原因是因為 —— 我真的有需要!\n",
            "請你們設身處地為我想一想！\n"
          ]
        }
      ],
      "source": [
        "!head {data_prefix+'.'+src_lang} -n 5\n",
        "!head {data_prefix+'.'+tgt_lang} -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRoE9UK7r1gY"
      },
      "source": [
        "## Preprocess files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3tzFwtnFrle3"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def strQ2B(ustring):\n",
        "    \"\"\"Full width -> half width\"\"\"\n",
        "    # reference:https://ithelp.ithome.com.tw/articles/10233122\n",
        "    ss = []\n",
        "    for s in ustring:\n",
        "        rstring = \"\"\n",
        "        for uchar in s:\n",
        "            inside_code = ord(uchar)\n",
        "            if inside_code == 12288:  # Full width space: direct conversion\n",
        "                inside_code = 32\n",
        "            elif (inside_code >= 65281 and inside_code <= 65374):  # Full width chars (except space) conversion\n",
        "                inside_code -= 65248\n",
        "            rstring += chr(inside_code)\n",
        "        ss.append(rstring)\n",
        "    return ''.join(ss)\n",
        "                \n",
        "def clean_s(s, lang):\n",
        "    if lang == 'en':\n",
        "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
        "        s = s.replace('-', '') # remove '-'\n",
        "        s = re.sub('([.,;!?()\\\"])', r' \\1 ', s) # keep punctuation\n",
        "    elif lang == 'zh':\n",
        "        s = strQ2B(s) # Q2B\n",
        "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
        "        s = s.replace(' ', '')\n",
        "        s = s.replace('—', '')\n",
        "        s = s.replace('“', '\"')\n",
        "        s = s.replace('”', '\"')\n",
        "        s = s.replace('_', '')\n",
        "        s = re.sub('([。,;!?()\\\"~「」])', r' \\1 ', s) # keep punctuation\n",
        "    s = ' '.join(s.strip().split())\n",
        "    return s\n",
        "\n",
        "def len_s(s, lang):\n",
        "    if lang == 'zh':\n",
        "        return len(s)\n",
        "    return len(s.split())\n",
        "\n",
        "def clean_corpus(prefix, l1, l2, ratio=9, max_len=1000, min_len=1):\n",
        "    if Path(f'{prefix}.clean.{l1}').exists() and Path(f'{prefix}.clean.{l2}').exists():\n",
        "        print(f'{prefix}.clean.{l1} & {l2} exists. skipping clean.')\n",
        "        return\n",
        "    with open(f'{prefix}.{l1}', 'r') as l1_in_f:\n",
        "        with open(f'{prefix}.{l2}', 'r') as l2_in_f:\n",
        "            with open(f'{prefix}.clean.{l1}', 'w') as l1_out_f:\n",
        "                with open(f'{prefix}.clean.{l2}', 'w') as l2_out_f:\n",
        "                    for s1 in l1_in_f:\n",
        "                        s1 = s1.strip()\n",
        "                        s2 = l2_in_f.readline().strip()\n",
        "                        s1 = clean_s(s1, l1)\n",
        "                        s2 = clean_s(s2, l2)\n",
        "                        s1_len = len_s(s1, l1)\n",
        "                        s2_len = len_s(s2, l2)\n",
        "                        if min_len > 0: # remove short sentence\n",
        "                            if s1_len < min_len or s2_len < min_len:\n",
        "                                continue\n",
        "                        if max_len > 0: # remove long sentence\n",
        "                            if s1_len > max_len or s2_len > max_len:\n",
        "                                continue\n",
        "                        if ratio > 0: # remove by ratio of length\n",
        "                            if s1_len/s2_len > ratio or s2_len/s1_len > ratio:\n",
        "                                continue\n",
        "                        print(s1, file=l1_out_f)\n",
        "                        print(s2, file=l2_out_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "h_i8b1PRr9Nf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/weiweichi/weichi/HW5/DATA/rawdata/ted2020/train_dev.raw.clean.en & zh exists. skipping clean.\n",
            "/home/weiweichi/weichi/HW5/DATA/rawdata/ted2020/test.raw.clean.en & zh exists. skipping clean.\n"
          ]
        }
      ],
      "source": [
        "clean_corpus(data_prefix, src_lang, tgt_lang)\n",
        "clean_corpus(test_prefix, src_lang, tgt_lang, ratio=-1, min_len=-1, max_len=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjT3XCy9r_rj",
        "outputId": "bdfb9b22-fc4d-4fb4-e244-647b903cea92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thank you so much , Chris .\n",
            "And it's truly a great honor to have the opportunity to come to this stage twice ; I'm extremely grateful .\n",
            "I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
            "And I say that sincerely , partly because I need that .\n",
            "Put yourselves in my position .\n",
            "非常謝謝你 , 克里斯 。 能有這個機會第二度踏上這個演講台\n",
            "真是一大榮幸 。 我非常感激 。\n",
            "這個研討會給我留下了極為深刻的印象 , 我想感謝大家對我之前演講的好評 。\n",
            "我是由衷的想這麼說 , 有部份原因是因為我真的有需要 !\n",
            "請你們設身處地為我想一想 !\n"
          ]
        }
      ],
      "source": [
        "!head {data_prefix+'.clean.'+src_lang} -n 5\n",
        "!head {data_prefix+'.clean.'+tgt_lang} -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKb4u67-sT_Z"
      },
      "source": [
        "## Split into train/valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AuFKeDz3sGHL"
      },
      "outputs": [],
      "source": [
        "valid_ratio = 0.01 # 3000~4000 would suffice\n",
        "train_ratio = 1 - valid_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QR2NVldqsXyY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train/valid splits exists. skipping split.\n"
          ]
        }
      ],
      "source": [
        "if (prefix/f'train.clean.{src_lang}').exists() \\\n",
        "and (prefix/f'train.clean.{tgt_lang}').exists() \\\n",
        "and (prefix/f'valid.clean.{src_lang}').exists() \\\n",
        "and (prefix/f'valid.clean.{tgt_lang}').exists():\n",
        "    print(f'train/valid splits exists. skipping split.')\n",
        "else:\n",
        "    line_num = sum(1 for line in open(f'{data_prefix}.clean.{src_lang}'))\n",
        "    labels = list(range(line_num))\n",
        "    random.shuffle(labels)\n",
        "    for lang in [src_lang, tgt_lang]:\n",
        "        train_f = open(os.path.join(data_dir, dataset_name, f'train.clean.{lang}'), 'w')\n",
        "        valid_f = open(os.path.join(data_dir, dataset_name, f'valid.clean.{lang}'), 'w')\n",
        "        count = 0\n",
        "        for line in open(f'{data_prefix}.clean.{lang}', 'r'):\n",
        "            if labels[count]/line_num < train_ratio:\n",
        "                train_f.write(line)\n",
        "            else:\n",
        "                valid_f.write(line)\n",
        "            count += 1\n",
        "        train_f.close()\n",
        "        valid_f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1rwQysTsdJq"
      },
      "source": [
        "## Subword Units \n",
        "Out of vocabulary (OOV) has been a major problem in machine translation. This can be alleviated by using subword units.\n",
        "- We will use the [sentencepiece](#kudo-richardson-2018-sentencepiece) package\n",
        "- select 'unigram' or 'byte-pair encoding (BPE)' algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ecwllsa7sZRA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/weiweichi/weichi/HW5/DATA/rawdata/ted2020/spm8000.model exists. skipping spm_train.\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "vocab_size = 8000\n",
        "if (prefix/f'spm{vocab_size}.model').exists():\n",
        "    print(f'{prefix}/spm{vocab_size}.model exists. skipping spm_train.')\n",
        "else:\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        input=','.join([f'{prefix}/train.clean.{src_lang}',\n",
        "                        f'{prefix}/valid.clean.{src_lang}',\n",
        "                        f'{prefix}/train.clean.{tgt_lang}',\n",
        "                        f'{prefix}/valid.clean.{tgt_lang}']),\n",
        "        model_prefix=prefix/f'spm{vocab_size}',\n",
        "        vocab_size=vocab_size,\n",
        "        character_coverage=1,\n",
        "        model_type='unigram', # 'bpe' works as well\n",
        "        input_sentence_size=1e6,\n",
        "        shuffle_input_sentence=True,\n",
        "        normalization_rule_name='nmt_nfkc_cf',\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lQPRNldqse_V"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/weiweichi/weichi/HW5/DATA/rawdata/ted2020/train.en exists. skipping spm_encode.\n",
            "/home/weiweichi/weichi/HW5/DATA/rawdata/ted2020/train.zh exists. skipping spm_encode.\n",
            "/home/weiweichi/weichi/HW5/DATA/rawdata/ted2020/valid.en exists. skipping spm_encode.\n",
            "/home/weiweichi/weichi/HW5/DATA/rawdata/ted2020/valid.zh exists. skipping spm_encode.\n",
            "/home/weiweichi/weichi/HW5/DATA/rawdata/ted2020/test.en exists. skipping spm_encode.\n",
            "/home/weiweichi/weichi/HW5/DATA/rawdata/ted2020/test.zh exists. skipping spm_encode.\n"
          ]
        }
      ],
      "source": [
        "spm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))\n",
        "in_tag = {\n",
        "    'train': 'train.clean',\n",
        "    'valid': 'valid.clean',\n",
        "    'test': 'test.raw.clean',\n",
        "}\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    for lang in [src_lang, tgt_lang]:\n",
        "        out_path = prefix/f'{split}.{lang}'\n",
        "        if out_path.exists():\n",
        "            print(f\"{out_path} exists. skipping spm_encode.\")\n",
        "        else:\n",
        "            with open(prefix/f'{split}.{lang}', 'w') as out_f:\n",
        "                with open(prefix/f'{in_tag[split]}.{lang}', 'r') as in_f:\n",
        "                    for line in in_f:\n",
        "                        line = line.strip()\n",
        "                        tok = spm_model.encode(line, out_type=str)\n",
        "                        print(' '.join(tok), file=out_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j6lXHjAsjXa",
        "outputId": "4a86a8a6-c461-441d-a933-d06ab071dc46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▁thank ▁you ▁so ▁much ▁, ▁chris ▁.\n",
            "▁and ▁it ' s ▁ t ru ly ▁a ▁great ▁ho n or ▁to ▁have ▁the ▁ op port un ity ▁to ▁come ▁to ▁this ▁st age ▁ t wi ce ▁; ▁i ' m ▁ex t re me ly ▁gr ate ful ▁.\n",
            "▁i ▁have ▁been ▁ bl ow n ▁away ▁by ▁this ▁con fer ence ▁, ▁and ▁i ▁want ▁to ▁thank ▁all ▁of ▁you ▁for ▁the ▁many ▁ ni ce ▁ com ment s ▁about ▁what ▁i ▁had ▁to ▁say ▁the ▁other ▁night ▁.\n",
            "▁and ▁i ▁say ▁that ▁since re ly ▁, ▁part ly ▁because ▁i ▁need ▁that ▁.\n",
            "▁put ▁your s el ve s ▁in ▁my ▁po s ition ▁.\n",
            "▁ 非常 謝 謝 你 ▁, ▁ 克 里 斯 ▁。 ▁ 能 有 這個 機會 第二 度 踏 上 這個 演講 台\n",
            "▁ 真 是 一 大 榮 幸 ▁。 ▁我 非常 感 激 ▁。\n",
            "▁這個 研 討 會 給我 留 下 了 極 為 深 刻 的 印 象 ▁, ▁我想 感 謝 大家 對我 之前 演講 的 好 評 ▁。\n",
            "▁我 是由 衷 的 想 這麼 說 ▁, ▁有 部份 原因 是因為 我 真的 有 需要 ▁!\n",
            "▁ 請 你們 設 身 處 地 為 我想 一 想 ▁!\n"
          ]
        }
      ],
      "source": [
        "!head {data_dir+'/'+dataset_name+'/train.'+src_lang} -n 5\n",
        "!head {data_dir+'/'+dataset_name+'/train.'+tgt_lang} -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59si_C0Wsms7"
      },
      "source": [
        "## Binarize the data with fairseq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-cHVLSpsknh",
        "outputId": "3447b375-ab55-4508-8fdd-b660c3f91ca0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATA/data-bin/ted2020 exists, will not overwrite!\n"
          ]
        }
      ],
      "source": [
        "binpath = Path('./DATA/data-bin', dataset_name)\n",
        "if binpath.exists():\n",
        "    print(binpath, \"exists, will not overwrite!\")\n",
        "else:\n",
        "    !python3 -m fairseq_cli.preprocess \\\n",
        "        --source-lang {src_lang}\\\n",
        "        --target-lang {tgt_lang}\\\n",
        "        --trainpref {prefix/'train'}\\\n",
        "        --validpref {prefix/'valid'}\\\n",
        "        --testpref {prefix/'test'}\\\n",
        "        --destdir {binpath}\\\n",
        "        --joined-dictionary\\\n",
        "        --workers 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szMuH1SWLPWA"
      },
      "source": [
        "# Configuration for experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5Luz3_tVLUxs"
      },
      "outputs": [],
      "source": [
        "config = Namespace(\n",
        "    datadir = \"./DATA/data-bin/ted2020\",\n",
        "    savedir = \"./checkpoints/transformer-bt\",\n",
        "    source_lang = \"zh\",\n",
        "    target_lang = \"en\",\n",
        "    \n",
        "    # cpu threads when fetching & processing data.\n",
        "    num_workers=2,  \n",
        "    # batch size in terms of tokens. gradient accumulation increases the effective batchsize.\n",
        "    max_tokens=8192,\n",
        "    accum_steps=2,\n",
        "    \n",
        "    # the lr s calculated from Noam lr scheduler. you can tune the maximum lr by this factor.\n",
        "    lr_factor=2.,\n",
        "    lr_warmup=4000,\n",
        "    \n",
        "    # clipping gradient norm helps alleviate gradient exploding\n",
        "    clip_norm=1.0,\n",
        "    \n",
        "    # maximum epochs for training\n",
        "    max_epoch=30,\n",
        "    start_epoch=1,\n",
        "    \n",
        "    # beam size for beam search\n",
        "    beam=5, \n",
        "    # generate sequences of maximum length ax + b, where x is the source length\n",
        "    max_len_a=1.2, \n",
        "    max_len_b=10, \n",
        "    # when decoding, post process sentence by removing sentencepiece symbols and jieba tokenization.\n",
        "    post_process = \"sentencepiece\",\n",
        "    \n",
        "    # checkpoints\n",
        "    keep_last_epochs=5,\n",
        "    resume=None, # if resume from checkpoint name (under config.savedir)\n",
        "    \n",
        "    # logging\n",
        "    use_wandb=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjrJFvyQLg86"
      },
      "source": [
        "# Logging\n",
        "- logging package logs ordinary messages\n",
        "- wandb logs the loss, bleu, etc. in the training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-ZiMyDWALbDk"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=\"INFO\", # \"DEBUG\" \"WARNING\" \"ERROR\"\n",
        "    stream=sys.stdout,\n",
        ")\n",
        "proj = \"hw5.seq2seq\"\n",
        "logger = logging.getLogger(proj)\n",
        "if config.use_wandb:\n",
        "    import wandb\n",
        "    wandb.init(project=proj, name=Path(config.savedir).stem, config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNoSkK45Lmqc"
      },
      "source": [
        "# CUDA Environments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqrsbmcoLqMl",
        "outputId": "b40fe6d7-bd8e-4cf2-a29f-e01442f7cf8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 02:34:59 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-04-07 02:34:59 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.913 GB ; name = NVIDIA GeForce GTX 1080 Ti              \n",
            "2022-04-07 02:34:59 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n"
          ]
        }
      ],
      "source": [
        "cuda_env = utils.CudaEnvironment()\n",
        "utils.CudaEnvironment.pretty_print_cuda_env_list([cuda_env])\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbJuBIHLLt2D"
      },
      "source": [
        "# Dataloading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpG4EBRLwe_"
      },
      "source": [
        "## We borrow the TranslationTask from fairseq\n",
        "* used to load the binarized data created above\n",
        "* well-implemented data iterator (dataloader)\n",
        "* built-in task.source_dictionary and task.target_dictionary are also handy\n",
        "* well-implemented beach search decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gSEy1uFLvVs",
        "outputId": "a7a4e866-1595-4d61-e680-0ff6543decd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 02:34:59 | INFO | fairseq.tasks.translation | [zh] dictionary: 7992 types\n",
            "2022-04-07 02:34:59 | INFO | fairseq.tasks.translation | [en] dictionary: 7992 types\n"
          ]
        }
      ],
      "source": [
        "from fairseq.tasks.translation import TranslationConfig, TranslationTask\n",
        "\n",
        "## setup task\n",
        "task_cfg = TranslationConfig(\n",
        "    data=config.datadir,\n",
        "    source_lang=config.source_lang,\n",
        "    target_lang=config.target_lang,\n",
        "    train_subset=\"train\",\n",
        "    required_seq_len_multiple=8,\n",
        "    dataset_impl=\"mmap\",\n",
        "    upsample_primary=1,\n",
        ")\n",
        "task = TranslationTask.setup_task(task_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR7Bhov7L4IU",
        "outputId": "b5d07fcc-7c94-47de-8f9d-4bd79ba7e924"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 02:34:59 | INFO | hw5.seq2seq | loading data for epoch 1\n",
            "2022-04-07 02:34:59 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./DATA/data-bin/ted2020/train.en-zh.zh\n",
            "2022-04-07 02:34:59 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./DATA/data-bin/ted2020/train.en-zh.en\n",
            "2022-04-07 02:34:59 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 train zh-en 390041 examples\n",
            "2022-04-07 02:34:59 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./DATA/data-bin/ted2020/valid.en-zh.zh\n",
            "2022-04-07 02:34:59 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./DATA/data-bin/ted2020/valid.en-zh.en\n",
            "2022-04-07 02:34:59 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 valid zh-en 3939 examples\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"loading data for epoch 1\")\n",
        "task.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\n",
        "task.load_dataset(split=\"valid\", epoch=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0BCEm_9L6ig",
        "outputId": "e03fdcf3-d4c9-4710-dbff-e94e0cce91a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 1,\n",
            " 'source': tensor([  53, 3134, 3107,   34,  408, 1223, 3895,  670,    9, 1038,  498,  148,\n",
            "        1689,  364,  129, 1050, 1103,   34,  408,  158,  607, 3268,  306,    9,\n",
            "        1105,    2]),\n",
            " 'target': tensor([  35,   22,  440,   27,    5,  850,  261,  172,    6,   29,  800,    6,\n",
            "         184,  164,   99, 1178,  374,  804,  257,   41,    5,  313,  868,   42,\n",
            "          27,   61, 1836,   18,  184, 1147, 1404,   21,    6,    6,  320,    7,\n",
            "           2])}\n",
            "'Source: 我們培植了一些果蠅它們的腦部被隨機地安置了一些可以光驅動的細胞'\n",
            "('Target: so we bred flies whose brains were more or less randomly peppered '\n",
            " 'with cells that were light addressable .')\n"
          ]
        }
      ],
      "source": [
        "sample = task.dataset(\"valid\")[1]\n",
        "pprint.pprint(sample)\n",
        "pprint.pprint(\n",
        "    \"Source: \" + \\\n",
        "    task.source_dictionary.string(\n",
        "        sample['source'],\n",
        "        config.post_process,\n",
        "    )\n",
        ")\n",
        "pprint.pprint(\n",
        "    \"Target: \" + \\\n",
        "    task.target_dictionary.string(\n",
        "        sample['target'],\n",
        "        config.post_process,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcfCVa2FMBSE"
      },
      "source": [
        "# Dataset iterator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBvc-B_6MKZM"
      },
      "source": [
        "* Controls every batch to contain no more than N tokens, which optimizes GPU memory efficiency\n",
        "* Shuffles the training set for every epoch\n",
        "* Ignore sentences exceeding maximum length\n",
        "* Pad all sentences in a batch to the same length, which enables parallel computing by GPU\n",
        "* Add eos and shift one token\n",
        "    - teacher forcing: to train the model to predict the next token based on prefix, we feed the right shifted target sequence as the decoder input.\n",
        "    - generally, prepending bos to the target would do the job (as shown below)\n",
        "![seq2seq](https://i.imgur.com/0zeDyuI.png)\n",
        "    - in fairseq however, this is done by moving the eos token to the begining. Empirically, this has the same effect. For instance:\n",
        "    ```\n",
        "    # output target (target) and Decoder input (prev_output_tokens): \n",
        "                   eos = 2\n",
        "                target = 419,  711,  238,  888,  792,   60,  968,    8,    2\n",
        "    prev_output_tokens = 2,  419,  711,  238,  888,  792,   60,  968,    8\n",
        "    ```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWFJFmCnMDXW",
        "outputId": "d5a62c5f-df9b-4e71-86ad-d27de323492d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 02:35:00 | WARNING | fairseq.tasks.fairseq_task | 2,565 samples have invalid sizes and will be skipped, max_positions=(20, 20), first few sample ids=[1811, 3245, 3729, 1898, 732, 226, 3630, 2905, 2594, 196]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'id': tensor([2001, 3666]),\n",
              " 'nsentences': 2,\n",
              " 'ntokens': 15,\n",
              " 'net_input': {'src_tokens': tensor([[   1,    5,  299, 1257,  843,  611,  413,    2],\n",
              "          [   1,   53,  968,  407,  177,  362,   33,    2]]),\n",
              "  'src_lengths': tensor([7, 7]),\n",
              "  'prev_output_tokens': tensor([[   2, 1166,  812,  542,  519,  538,    7,    1],\n",
              "          [   2,  151,  741,   22,  657,  718,   18,   33]])},\n",
              " 'target': tensor([[1166,  812,  542,  519,  538,    7,    2,    1],\n",
              "         [ 151,  741,   22,  657,  718,   18,   33,    2]])}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def load_data_iterator(task, split, epoch=1, max_tokens=4000, num_workers=1, cached=True):\n",
        "    batch_iterator = task.get_batch_iterator(\n",
        "        dataset=task.dataset(split),\n",
        "        max_tokens=max_tokens,\n",
        "        max_sentences=None,\n",
        "        max_positions=utils.resolve_max_positions(\n",
        "            task.max_positions(),\n",
        "            max_tokens,\n",
        "        ),\n",
        "        ignore_invalid_inputs=True,\n",
        "        seed=seed,\n",
        "        num_workers=num_workers,\n",
        "        epoch=epoch,\n",
        "        disable_iterator_cache=not cached,\n",
        "        # Set this to False to speed up. However, if set to False, changing max_tokens beyond \n",
        "        # first call of this method has no effect. \n",
        "    )\n",
        "    return batch_iterator\n",
        "\n",
        "demo_epoch_obj = load_data_iterator(task, \"valid\", epoch=1, max_tokens=20, num_workers=1, cached=False)\n",
        "demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True)\n",
        "sample = next(demo_iter)\n",
        "sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p86K-0g7Me4M"
      },
      "source": [
        "* each batch is a python dict, with string key and Tensor value. Contents are described below:\n",
        "```python\n",
        "batch = {\n",
        "    \"id\": id, # id for each example \n",
        "    \"nsentences\": len(samples), # batch size (sentences)\n",
        "    \"ntokens\": ntokens, # batch size (tokens)\n",
        "    \"net_input\": {\n",
        "        \"src_tokens\": src_tokens, # sequence in source language\n",
        "        \"src_lengths\": src_lengths, # sequence length of each example before padding\n",
        "        \"prev_output_tokens\": prev_output_tokens, # right shifted target, as mentioned above.\n",
        "    },\n",
        "    \"target\": target, # target sequence\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EyDBE5ZMkFZ"
      },
      "source": [
        "# Model Architecture\n",
        "* We again inherit fairseq's encoder, decoder and model, so that in the testing phase we can directly leverage fairseq's beam search decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Hzh74qLIMfW_"
      },
      "outputs": [],
      "source": [
        "from fairseq.models import (\n",
        "    FairseqEncoder, \n",
        "    FairseqIncrementalDecoder,\n",
        "    FairseqEncoderDecoderModel\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDAPmxjRNEEL"
      },
      "source": [
        "## Seq2Seq\n",
        "- Composed of **Encoder** and **Decoder**\n",
        "- Recieves inputs and pass to **Encoder** \n",
        "- Pass the outputs from **Encoder** to **Decoder**\n",
        "- **Decoder** will decode according to outputs of previous timesteps as well as **Encoder** outputs  \n",
        "- Once done decoding, return the **Decoder** outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "oRwKdLa0NEU6"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(FairseqEncoderDecoderModel):\n",
        "    def __init__(self, args, encoder, decoder):\n",
        "        super().__init__(encoder, decoder)\n",
        "        self.args = args\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        src_tokens,\n",
        "        src_lengths,\n",
        "        prev_output_tokens,\n",
        "        return_all_hiddens: bool = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Run the forward pass for an encoder-decoder model.\n",
        "        \"\"\"\n",
        "        encoder_out = self.encoder(\n",
        "            src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
        "        )\n",
        "        logits, extra = self.decoder(\n",
        "            prev_output_tokens,\n",
        "            encoder_out=encoder_out,\n",
        "            src_lengths=src_lengths,\n",
        "            return_all_hiddens=return_all_hiddens,\n",
        "        )\n",
        "        return logits, extra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu3C2JfqNHzk"
      },
      "source": [
        "# Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "nyI9FOx-NJ2m"
      },
      "outputs": [],
      "source": [
        "# # HINT: transformer architecture\n",
        "from fairseq.models.transformer import (\n",
        "    TransformerEncoder, \n",
        "    TransformerDecoder,\n",
        ")\n",
        "\n",
        "def build_model(args, task):\n",
        "    \"\"\" build a model instance based on hyperparameters \"\"\"\n",
        "    src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
        "\n",
        "    # token embeddings\n",
        "    encoder_embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, src_dict.pad())\n",
        "    decoder_embed_tokens = nn.Embedding(len(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())\n",
        "    \n",
        "    # encoder decoder\n",
        "    # HINT: TODO: switch to TransformerEncoder & TransformerDecoder\n",
        "    encoder = TransformerEncoder(args, src_dict, encoder_embed_tokens)\n",
        "    decoder = TransformerDecoder(args, tgt_dict, decoder_embed_tokens)\n",
        "\n",
        "    # sequence to sequence model\n",
        "    model = Seq2Seq(args, encoder, decoder)\n",
        "    \n",
        "    # initialization for seq2seq model is important, requires extra handling\n",
        "    def init_params(module):\n",
        "        from fairseq.modules import MultiheadAttention\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        if isinstance(module, MultiheadAttention):\n",
        "            module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "        if isinstance(module, nn.RNNBase):\n",
        "            for name, param in module.named_parameters():\n",
        "                if \"weight\" in name or \"bias\" in name:\n",
        "                    param.data.uniform_(-0.1, 0.1)\n",
        "            \n",
        "    # weight initialization\n",
        "    model.apply(init_params)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce5n4eS7NQNy"
      },
      "source": [
        "## Architecture Related Configuration\n",
        "\n",
        "For strong baseline, please refer to the hyperparameters for *transformer-base* in Table 3 in [Attention is all you need](#vaswani2017)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Cyn30VoGNT6N"
      },
      "outputs": [],
      "source": [
        "arch_args = Namespace(\n",
        "    encoder_embed_dim=480,\n",
        "    encoder_ffn_embed_dim=2048,\n",
        "    encoder_layers=5,\n",
        "    decoder_embed_dim=480,\n",
        "    decoder_ffn_embed_dim=2048,\n",
        "    decoder_layers=5,\n",
        "    share_decoder_input_output_embed=True,\n",
        "    dropout=0.1,\n",
        ")\n",
        "\n",
        "# HINT: these patches on parameters for Transformer\n",
        "def add_transformer_args(args):\n",
        "    args.encoder_attention_heads=12\n",
        "    args.encoder_normalize_before=True\n",
        "    \n",
        "    args.decoder_attention_heads=12\n",
        "    args.decoder_normalize_before=True\n",
        "    \n",
        "    args.activation_fn=\"relu\"\n",
        "    args.max_source_positions=1024\n",
        "    args.max_target_positions=1024\n",
        "    \n",
        "    # patches on default parameters for Transformer (those not set above)\n",
        "    from fairseq.models.transformer import base_architecture\n",
        "    base_architecture(arch_args)\n",
        "\n",
        "add_transformer_args(arch_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Nbb76QLCNZZZ"
      },
      "outputs": [],
      "source": [
        "if config.use_wandb:\n",
        "    wandb.config.update(vars(arch_args))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZWfxsCDNatH",
        "outputId": "ce69bfe3-52b7-4e6b-afb5-2b3a7ac37149"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 02:35:01 | INFO | hw5.seq2seq | Seq2Seq(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(7992, 480, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=480, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=480, bias=True)\n",
            "        (final_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=480, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=480, bias=True)\n",
            "        (final_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=480, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=480, bias=True)\n",
            "        (final_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=480, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=480, bias=True)\n",
            "        (final_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=480, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=480, bias=True)\n",
            "        (final_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(7992, 480, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=480, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=480, bias=True)\n",
            "        (final_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=480, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=480, bias=True)\n",
            "        (final_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=480, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=480, bias=True)\n",
            "        (final_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=480, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=480, bias=True)\n",
            "        (final_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "          (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=480, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=480, bias=True)\n",
            "        (final_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=480, out_features=7992, bias=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = build_model(arch_args, task)\n",
        "logger.info(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHll7GRNNdqc"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUB9f1WCNgMH"
      },
      "source": [
        "## Loss: Label Smoothing Regularization\n",
        "* let the model learn to generate less concentrated distribution, and prevent over-confidence\n",
        "* sometimes the ground truth may not be the only answer. thus, when calculating loss, we reserve some probability for incorrect labels\n",
        "* avoids overfitting\n",
        "\n",
        "code [source](https://fairseq.readthedocs.io/en/latest/_modules/fairseq/criterions/label_smoothed_cross_entropy.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "IgspdJn0NdYF"
      },
      "outputs": [],
      "source": [
        "class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
        "    def __init__(self, smoothing, ignore_index=None, reduce=True):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.ignore_index = ignore_index\n",
        "        self.reduce = reduce\n",
        "    \n",
        "    def forward(self, lprobs, target):\n",
        "        if target.dim() == lprobs.dim() - 1:\n",
        "            target = target.unsqueeze(-1)\n",
        "        # nll: Negative log likelihood，the cross-entropy when target is one-hot. following line is same as F.nll_loss\n",
        "        nll_loss = -lprobs.gather(dim=-1, index=target)\n",
        "        #  reserve some probability for other labels. thus when calculating cross-entropy, \n",
        "        # equivalent to summing the log probs of all labels\n",
        "        smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
        "        if self.ignore_index is not None:\n",
        "            pad_mask = target.eq(self.ignore_index)\n",
        "            nll_loss.masked_fill_(pad_mask, 0.0)\n",
        "            smooth_loss.masked_fill_(pad_mask, 0.0)\n",
        "        else:\n",
        "            nll_loss = nll_loss.squeeze(-1)\n",
        "            smooth_loss = smooth_loss.squeeze(-1)\n",
        "        if self.reduce:\n",
        "            nll_loss = nll_loss.sum()\n",
        "            smooth_loss = smooth_loss.sum()\n",
        "        # when calculating cross-entropy, add the loss of other labels\n",
        "        eps_i = self.smoothing / lprobs.size(-1)\n",
        "        loss = (1.0 - self.smoothing) * nll_loss + eps_i * smooth_loss\n",
        "        return loss\n",
        "\n",
        "# generally, 0.1 is good enough\n",
        "criterion = LabelSmoothedCrossEntropyCriterion(\n",
        "    smoothing=0.1,\n",
        "    ignore_index=task.target_dictionary.pad(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRalDto2NkJJ"
      },
      "source": [
        "## Optimizer: Adam + lr scheduling\n",
        "Inverse square root scheduling is important to the stability when training Transformer. It's later used on RNN as well.\n",
        "Update the learning rate according to the following equation. Linearly increase the first stage, then decay proportionally to the inverse square root of timestep.\n",
        "$$lrate = d_{\\text{model}}^{-0.5}\\cdot\\min({step\\_num}^{-0.5},{step\\_num}\\cdot{warmup\\_steps}^{-1.5})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "sS7tQj1ROBYm"
      },
      "outputs": [],
      "source": [
        "def get_rate(d_model, step_num, warmup_step):\n",
        "    # TODO: Change lr from constant to the equation shown above\n",
        "    return d_model**(-0.5) * min(step_num ** (-0.5), step_num * warmup_step ** (-1.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "J8hoAjHPNkh3"
      },
      "outputs": [],
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    @property\n",
        "    def param_groups(self):\n",
        "        return self.optimizer.param_groups\n",
        "        \n",
        "    def multiply_grads(self, c):\n",
        "        \"\"\"Multiplies grads by a constant *c*.\"\"\"                \n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is not None:\n",
        "                    p.grad.data.mul_(c)\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return 0 if not step else self.factor * get_rate(self.model_size, step, self.warmup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFJlkOMONsc6"
      },
      "source": [
        "## Scheduling Visualized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "A135fwPCNrQs",
        "outputId": "7074381b-f215-463a-da7d-3422dc7b675d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "optimizer = NoamOpt(\n",
        "    model_size=arch_args.encoder_embed_dim, \n",
        "    factor=config.lr_factor, \n",
        "    warmup=config.lr_warmup, \n",
        "    optimizer=torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001))\n",
        "plt.plot(np.arange(1, 100000), [optimizer.rate(i) for i in range(1, 100000)])\n",
        "plt.legend([f\"{optimizer.model_size}:{optimizer.warmup}\"])\n",
        "None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOR0g-cVO5ZO"
      },
      "source": [
        "# Training Procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-0ZjbK3O8Iv"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "foal3xM1O404"
      },
      "outputs": [],
      "source": [
        "from fairseq.data import iterators\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "def train_one_epoch(epoch_itr, model, task, criterion, optimizer, accum_steps=1):\n",
        "    itr = epoch_itr.next_epoch_itr(shuffle=True)\n",
        "    itr = iterators.GroupedIterator(itr, accum_steps) # gradient accumulation: update every accum_steps samples\n",
        "    \n",
        "    stats = {\"loss\": []}\n",
        "    scaler = GradScaler() # automatic mixed precision (amp) \n",
        "    \n",
        "    model.train()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"train epoch {epoch_itr.epoch}\", leave=False)\n",
        "    for samples in progress:\n",
        "        model.zero_grad()\n",
        "        accum_loss = 0\n",
        "        sample_size = 0\n",
        "        # gradient accumulation: update every accum_steps samples\n",
        "        for i, sample in enumerate(samples):\n",
        "            if i == 1:\n",
        "                # emptying the CUDA cache after the first step can reduce the chance of OOM\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "            target = sample[\"target\"]\n",
        "            sample_size_i = sample[\"ntokens\"]\n",
        "            sample_size += sample_size_i\n",
        "            \n",
        "            # mixed precision training\n",
        "            with autocast():\n",
        "                net_output = model.forward(**sample[\"net_input\"])\n",
        "                lprobs = F.log_softmax(net_output[0], -1)            \n",
        "                loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1))\n",
        "                \n",
        "                # logging\n",
        "                accum_loss += loss.item()\n",
        "                # back-prop\n",
        "                scaler.scale(loss).backward()                \n",
        "        \n",
        "        scaler.unscale_(optimizer)\n",
        "        optimizer.multiply_grads(1 / (sample_size or 1.0)) # (sample_size or 1.0) handles the case of a zero gradient\n",
        "        gnorm = nn.utils.clip_grad_norm_(model.parameters(), config.clip_norm) # grad norm clipping prevents gradient exploding\n",
        "        \n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        # logging\n",
        "        loss_print = accum_loss/sample_size\n",
        "        stats[\"loss\"].append(loss_print)\n",
        "        progress.set_postfix(loss=loss_print)\n",
        "        if config.use_wandb:\n",
        "            wandb.log({\n",
        "                \"train/loss\": loss_print,\n",
        "                \"train/grad_norm\": gnorm.item(),\n",
        "                \"train/lr\": optimizer.rate(),\n",
        "                \"train/sample_size\": sample_size,\n",
        "            })\n",
        "        \n",
        "    loss_print = np.mean(stats[\"loss\"])\n",
        "    logger.info(f\"training loss: {loss_print:.4f}\")\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt1lX3DRO_yU"
      },
      "source": [
        "## Validation & Inference\n",
        "To prevent overfitting, validation is required every epoch to validate the performance on unseen data.\n",
        "- the procedure is essensially same as training, with the addition of inference step\n",
        "- after validation we can save the model weights\n",
        "\n",
        "Validation loss alone cannot describe the actual performance of the model\n",
        "- Directly produce translation hypotheses based on current model, then calculate BLEU with the reference translation\n",
        "- We can also manually examine the hypotheses' quality\n",
        "- We use fairseq's sequence generator for beam search to generate translation hypotheses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "2og80HYQPAKq"
      },
      "outputs": [],
      "source": [
        "# fairseq's beam search generator\n",
        "# given model and input seqeunce, produce translation hypotheses by beam search\n",
        "sequence_generator = task.build_generator([model], config)\n",
        "\n",
        "def decode(toks, dictionary):\n",
        "    # convert from Tensor to human readable sentence\n",
        "    s = dictionary.string(\n",
        "        toks.int().cpu(),\n",
        "        config.post_process,\n",
        "    )\n",
        "    return s if s else \"<unk>\"\n",
        "\n",
        "def inference_step(sample, model):\n",
        "    gen_out = sequence_generator.generate([model], sample)\n",
        "    srcs = []\n",
        "    hyps = []\n",
        "    refs = []\n",
        "    for i in range(len(gen_out)):\n",
        "        # for each sample, collect the input, hypothesis and reference, later be used to calculate BLEU\n",
        "        srcs.append(decode(\n",
        "            utils.strip_pad(sample[\"net_input\"][\"src_tokens\"][i], task.source_dictionary.pad()), \n",
        "            task.source_dictionary,\n",
        "        ))\n",
        "        hyps.append(decode(\n",
        "            gen_out[i][0][\"tokens\"], # 0 indicates using the top hypothesis in beam\n",
        "            task.target_dictionary,\n",
        "        ))\n",
        "        refs.append(decode(\n",
        "            utils.strip_pad(sample[\"target\"][i], task.target_dictionary.pad()), \n",
        "            task.target_dictionary,\n",
        "        ))\n",
        "    return srcs, hyps, refs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "y1o7LeDkPDsd"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import sacrebleu\n",
        "\n",
        "def validate(model, task, criterion, log_to_wandb=True):\n",
        "    logger.info('begin validation')\n",
        "    itr = load_data_iterator(task, \"valid\", 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
        "    \n",
        "    stats = {\"loss\":[], \"bleu\": 0, \"srcs\":[], \"hyps\":[], \"refs\":[]}\n",
        "    srcs = []\n",
        "    hyps = []\n",
        "    refs = []\n",
        "    \n",
        "    model.eval()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"validation\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(progress):\n",
        "            # validation loss\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "            net_output = model.forward(**sample[\"net_input\"])\n",
        "\n",
        "            lprobs = F.log_softmax(net_output[0], -1)\n",
        "            target = sample[\"target\"]\n",
        "            sample_size = sample[\"ntokens\"]\n",
        "            loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1)) / sample_size\n",
        "            progress.set_postfix(valid_loss=loss.item())\n",
        "            stats[\"loss\"].append(loss)\n",
        "            \n",
        "            # do inference\n",
        "            s, h, r = inference_step(sample, model)\n",
        "            srcs.extend(s)\n",
        "            hyps.extend(h)\n",
        "            refs.extend(r)\n",
        "            \n",
        "    tok = 'zh' if task.cfg.target_lang == 'zh' else '13a'\n",
        "    stats[\"loss\"] = torch.stack(stats[\"loss\"]).mean().item()\n",
        "    stats[\"bleu\"] = sacrebleu.corpus_bleu(hyps, [refs], tokenize=tok) # 計算BLEU score\n",
        "    stats[\"srcs\"] = srcs\n",
        "    stats[\"hyps\"] = hyps\n",
        "    stats[\"refs\"] = refs\n",
        "    \n",
        "    if config.use_wandb and log_to_wandb:\n",
        "        wandb.log({\n",
        "            \"valid/loss\": stats[\"loss\"],\n",
        "            \"valid/bleu\": stats[\"bleu\"].score,\n",
        "        }, commit=False)\n",
        "    \n",
        "    showid = np.random.randint(len(hyps))\n",
        "    logger.info(\"example source: \" + srcs[showid])\n",
        "    logger.info(\"example hypothesis: \" + hyps[showid])\n",
        "    logger.info(\"example reference: \" + refs[showid])\n",
        "    \n",
        "    # show bleu results\n",
        "    logger.info(f\"validation loss:\\t{stats['loss']:.4f}\")\n",
        "    logger.info(stats[\"bleu\"].format())\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sRF6nd4PGEE"
      },
      "source": [
        "# Save and Load Model Weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "edBuLlkuPGr9"
      },
      "outputs": [],
      "source": [
        "def validate_and_save(model, task, criterion, optimizer, epoch, save=True):   \n",
        "    stats = validate(model, task, criterion)\n",
        "    bleu = stats['bleu']\n",
        "    loss = stats['loss']\n",
        "    if save:\n",
        "        # save epoch checkpoints\n",
        "        savedir = Path(config.savedir).absolute()\n",
        "        savedir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        check = {\n",
        "            \"model\": model.state_dict(),\n",
        "            \"stats\": {\"bleu\": bleu.score, \"loss\": loss},\n",
        "            \"optim\": {\"step\": optimizer._step}\n",
        "        }\n",
        "        torch.save(check, savedir/f\"checkpoint{epoch}.pt\")\n",
        "        shutil.copy(savedir/f\"checkpoint{epoch}.pt\", savedir/f\"checkpoint_last.pt\")\n",
        "        logger.info(f\"saved epoch checkpoint: {savedir}/checkpoint{epoch}.pt\")\n",
        "    \n",
        "        # save epoch samples\n",
        "        with open(savedir/f\"samples{epoch}.{config.source_lang}-{config.target_lang}.txt\", \"w\") as f:\n",
        "            for s, h in zip(stats[\"srcs\"], stats[\"hyps\"]):\n",
        "                f.write(f\"{s}\\t{h}\\n\")\n",
        "\n",
        "        # get best valid bleu    \n",
        "        if getattr(validate_and_save, \"best_bleu\", 0) < bleu.score:\n",
        "            validate_and_save.best_bleu = bleu.score\n",
        "            torch.save(check, savedir/f\"checkpoint_best.pt\")\n",
        "            \n",
        "        del_file = savedir / f\"checkpoint{epoch - config.keep_last_epochs}.pt\"\n",
        "        if del_file.exists():\n",
        "            del_file.unlink()\n",
        "    \n",
        "    return stats\n",
        "\n",
        "def try_load_checkpoint(model, optimizer=None, name=None):\n",
        "    name = name if name else \"checkpoint_last.pt\"\n",
        "    checkpath = Path(config.savedir)/name\n",
        "    if checkpath.exists():\n",
        "        check = torch.load(checkpath)\n",
        "        model.load_state_dict(check[\"model\"])\n",
        "        stats = check[\"stats\"]\n",
        "        step = \"unknown\"\n",
        "        if optimizer != None:\n",
        "            optimizer._step = step = check[\"optim\"][\"step\"]\n",
        "        logger.info(f\"loaded checkpoint {checkpath}: step={step} loss={stats['loss']} bleu={stats['bleu']}\")\n",
        "    else:\n",
        "        logger.info(f\"no checkpoints found at {checkpath}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyIFpibfPJ5u"
      },
      "source": [
        "# Main\n",
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "hu7RZbCUPKQr"
      },
      "outputs": [],
      "source": [
        "model = model.to(device=device)\n",
        "criterion = criterion.to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xxlJxU2PeAo",
        "outputId": "151dd30f-d0a4-45ef-eb20-058c827dad43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 02:35:04 | INFO | hw5.seq2seq | task: TranslationTask\n",
            "2022-04-07 02:35:04 | INFO | hw5.seq2seq | encoder: TransformerEncoder\n",
            "2022-04-07 02:35:04 | INFO | hw5.seq2seq | decoder: TransformerDecoder\n",
            "2022-04-07 02:35:04 | INFO | hw5.seq2seq | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2022-04-07 02:35:04 | INFO | hw5.seq2seq | optimizer: NoamOpt\n",
            "2022-04-07 02:35:04 | INFO | hw5.seq2seq | num. model params: 41,237,120 (num. trained: 41,237,120)\n",
            "2022-04-07 02:35:04 | INFO | hw5.seq2seq | max tokens per batch = 8192, accumulate steps = 2\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"task: {}\".format(task.__class__.__name__))\n",
        "logger.info(\"encoder: {}\".format(model.encoder.__class__.__name__))\n",
        "logger.info(\"decoder: {}\".format(model.decoder.__class__.__name__))\n",
        "logger.info(\"criterion: {}\".format(criterion.__class__.__name__))\n",
        "logger.info(\"optimizer: {}\".format(optimizer.__class__.__name__))\n",
        "logger.info(\n",
        "    \"num. model params: {:,} (num. trained: {:,})\".format(\n",
        "        sum(p.numel() for p in model.parameters()),\n",
        "        sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
        "    )\n",
        ")\n",
        "logger.info(f\"max tokens per batch = {config.max_tokens}, accumulate steps = {config.accum_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "MSPRqpQUPfaX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 02:35:05 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[326738]\n",
            "2022-04-07 02:35:05 | INFO | hw5.seq2seq | no checkpoints found at checkpoints/transformer-bt/checkpoint_last.pt!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "544d17e826a8407eaaba3768e8b83c9a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 1:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 02:45:15 | INFO | hw5.seq2seq | training loss: 5.6764\n",
            "2022-04-07 02:45:15 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "edfa2a2db3114655a55dc0e2f1745af0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/weiweichi/.local/lib/python3.8/site-packages/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/home/weiweichi/.local/lib/python3.8/site-packages/fairseq/sequence_generator.py:657: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = idx // beam_size\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 02:45:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 02:45:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 02:45:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 02:45:51 | INFO | hw5.seq2seq | example source: 艾曼達 , 能否請您分享您如何失去手臂的 ?\n",
            "2022-04-07 02:45:51 | INFO | hw5.seq2seq | example hypothesis: johhn , how can you get to tell you how do you get your hand ?\n",
            "2022-04-07 02:45:51 | INFO | hw5.seq2seq | example reference: so amanda , would you please tell us how you lost your arm ?\n",
            "2022-04-07 02:45:51 | INFO | hw5.seq2seq | validation loss:\t4.3532\n",
            "2022-04-07 02:45:51 | INFO | hw5.seq2seq | BLEU = 3.04 27.3/5.4/1.5/0.4 (BP = 0.979 ratio = 0.979 hyp_len = 74559 ref_len = 76142)\n",
            "2022-04-07 02:45:51 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint1.pt\n",
            "2022-04-07 02:45:51 | INFO | hw5.seq2seq | end of epoch 1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d52d558f5f0e4bea92a4cd0c72cc5f1b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 2:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 02:56:01 | INFO | hw5.seq2seq | training loss: 3.9279\n",
            "2022-04-07 02:56:01 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83572b82764142a9966fb0d11472211e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 02:56:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 02:56:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 02:56:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 02:56:33 | INFO | hw5.seq2seq | example source: 在未來兩年 , 我的工作是設計一個總體計畫並於十年內執行本計劃-當然 , 這需要眾人攜手合作 。\n",
            "2022-04-07 02:56:33 | INFO | hw5.seq2seq | example hypothesis: in two years , my job is designed to be a summer , and of course , it's a requirement project , and of course , it's a required to work .\n",
            "2022-04-07 02:56:33 | INFO | hw5.seq2seq | example reference: that is my job for the next two years , to design an entire master plan , and then for the next 10 years to implement it of course , with so many other people .\n",
            "2022-04-07 02:56:33 | INFO | hw5.seq2seq | validation loss:\t3.4911\n",
            "2022-04-07 02:56:33 | INFO | hw5.seq2seq | BLEU = 9.02 39.5/13.3/5.6/2.5 (BP = 0.977 ratio = 0.977 hyp_len = 74387 ref_len = 76142)\n",
            "2022-04-07 02:56:35 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint2.pt\n",
            "2022-04-07 02:56:36 | INFO | hw5.seq2seq | end of epoch 2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0dc2cb3956f4ebfa3641ea7342bfbbf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 3:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 03:06:45 | INFO | hw5.seq2seq | training loss: 3.3538\n",
            "2022-04-07 03:06:45 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94fb56ccd6ed4eb89524e2edd4009e62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 03:07:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 03:07:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 03:07:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 03:07:11 | INFO | hw5.seq2seq | example source: 如果我們能夠改變這個動力 , 首先 , 創造人口密度更高更適合居住的城市...\n",
            "2022-04-07 03:07:11 | INFO | hw5.seq2seq | example hypothesis: if we can change this , first of all , create density , create dense cities .\n",
            "2022-04-07 03:07:11 | INFO | hw5.seq2seq | example reference: if we can change the dynamic , by first of all creating cities that are denser and more livable . . .\n",
            "2022-04-07 03:07:11 | INFO | hw5.seq2seq | validation loss:\t3.1678\n",
            "2022-04-07 03:07:11 | INFO | hw5.seq2seq | BLEU = 10.75 52.6/21.1/10.0/4.9 (BP = 0.703 ratio = 0.739 hyp_len = 56287 ref_len = 76142)\n",
            "2022-04-07 03:07:13 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint3.pt\n",
            "2022-04-07 03:07:15 | INFO | hw5.seq2seq | end of epoch 3\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f08e496a1b0e42e59d40f3d25a70584b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 4:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 03:17:25 | INFO | hw5.seq2seq | training loss: 3.0960\n",
            "2022-04-07 03:17:25 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a53d26f3e8345bb85c04508ba3c8563",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 03:17:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 03:17:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 03:17:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 03:17:53 | INFO | hw5.seq2seq | example source: 他將與我同行 ,\n",
            "2022-04-07 03:17:53 | INFO | hw5.seq2seq | example hypothesis: he's going to work with me .\n",
            "2022-04-07 03:17:53 | INFO | hw5.seq2seq | example reference: he is coming with me .\n",
            "2022-04-07 03:17:53 | INFO | hw5.seq2seq | validation loss:\t3.0139\n",
            "2022-04-07 03:17:53 | INFO | hw5.seq2seq | BLEU = 13.43 50.6/21.4/10.5/5.4 (BP = 0.853 ratio = 0.863 hyp_len = 65703 ref_len = 76142)\n",
            "2022-04-07 03:17:54 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint4.pt\n",
            "2022-04-07 03:17:56 | INFO | hw5.seq2seq | end of epoch 4\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c45a2c1f7b9944cf8aa1f2d3076bf5cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 5:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 03:28:06 | INFO | hw5.seq2seq | training loss: 2.9631\n",
            "2022-04-07 03:28:06 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1cb617587ce4b52ae6caf737b241a84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 03:28:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 03:28:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 03:28:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 03:28:37 | INFO | hw5.seq2seq | example source: 這叫快速通關走道或vip票\n",
            "2022-04-07 03:28:37 | INFO | hw5.seq2seq | example hypothesis: this is called quick traffic or vip .\n",
            "2022-04-07 03:28:37 | INFO | hw5.seq2seq | example reference: they call them fast track or vip tickets .\n",
            "2022-04-07 03:28:37 | INFO | hw5.seq2seq | validation loss:\t2.9447\n",
            "2022-04-07 03:28:37 | INFO | hw5.seq2seq | BLEU = 15.04 48.6/21.0/10.4/5.4 (BP = 0.972 ratio = 0.972 hyp_len = 74048 ref_len = 76142)\n",
            "2022-04-07 03:28:39 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint5.pt\n",
            "2022-04-07 03:28:40 | INFO | hw5.seq2seq | end of epoch 5\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d0c8227fcea4fc6b96cfd5389496e4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 6:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 03:38:49 | INFO | hw5.seq2seq | training loss: 2.8529\n",
            "2022-04-07 03:38:49 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5f3d16e6d2d4bb089f3f7d086dd8c5b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 03:39:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 03:39:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 03:39:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 03:39:19 | INFO | hw5.seq2seq | example source: 可能我們會想要把它用在某些結構上之類的 。\n",
            "2022-04-07 03:39:19 | INFO | hw5.seq2seq | example hypothesis: maybe we're going to want to use it to some structures or something like that .\n",
            "2022-04-07 03:39:19 | INFO | hw5.seq2seq | example reference: probably we're going to want it for some structures , and so on .\n",
            "2022-04-07 03:39:19 | INFO | hw5.seq2seq | validation loss:\t2.8229\n",
            "2022-04-07 03:39:19 | INFO | hw5.seq2seq | BLEU = 16.10 52.0/23.2/12.0/6.5 (BP = 0.921 ratio = 0.924 hyp_len = 70330 ref_len = 76142)\n",
            "2022-04-07 03:39:20 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint6.pt\n",
            "2022-04-07 03:39:22 | INFO | hw5.seq2seq | end of epoch 6\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e216bef6503d4f359b761a01ee725e46",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 7:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 03:49:30 | INFO | hw5.seq2seq | training loss: 2.7307\n",
            "2022-04-07 03:49:30 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d96168f04f8469290ef8787d5cf0894",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 03:50:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 03:50:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 03:50:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 03:50:01 | INFO | hw5.seq2seq | example source: 最後還有支付能力的問題\n",
            "2022-04-07 03:50:01 | INFO | hw5.seq2seq | example hypothesis: and finally , there's a problem of payment .\n",
            "2022-04-07 03:50:01 | INFO | hw5.seq2seq | example reference: and then there's finally there's the affordability question .\n",
            "2022-04-07 03:50:01 | INFO | hw5.seq2seq | validation loss:\t2.7654\n",
            "2022-04-07 03:50:01 | INFO | hw5.seq2seq | BLEU = 16.85 50.0/22.6/11.7/6.3 (BP = 0.990 ratio = 0.990 hyp_len = 75393 ref_len = 76142)\n",
            "2022-04-07 03:50:03 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint7.pt\n",
            "2022-04-07 03:50:04 | INFO | hw5.seq2seq | end of epoch 7\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f93d83d85fb1408e81001a76ac8ee708",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 8:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 04:00:12 | INFO | hw5.seq2seq | training loss: 2.6447\n",
            "2022-04-07 04:00:12 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f68970f40324a9187143ed67934cd32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 04:00:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 04:00:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 04:00:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 04:00:42 | INFO | hw5.seq2seq | example source: 這相當於每年從海裡撈出全體中國人的重量 。\n",
            "2022-04-07 04:00:42 | INFO | hw5.seq2seq | example hypothesis: that's the equivalent of the weight of the chinese from the ocean every year .\n",
            "2022-04-07 04:00:42 | INFO | hw5.seq2seq | example reference: that's the equivalent of the human weight of china taken out of the sea every year .\n",
            "2022-04-07 04:00:42 | INFO | hw5.seq2seq | validation loss:\t2.7043\n",
            "2022-04-07 04:00:42 | INFO | hw5.seq2seq | BLEU = 18.17 53.5/25.1/13.5/7.5 (BP = 0.946 ratio = 0.948 hyp_len = 72149 ref_len = 76142)\n",
            "2022-04-07 04:00:44 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint8.pt\n",
            "2022-04-07 04:00:45 | INFO | hw5.seq2seq | end of epoch 8\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd1af4b92cec47498cbd02e2fc5ccae6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 9:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 04:10:56 | INFO | hw5.seq2seq | training loss: 2.5734\n",
            "2022-04-07 04:10:56 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f19add49ae8461a8f34dcad5584295b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 04:11:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 04:11:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 04:11:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 04:11:23 | INFO | hw5.seq2seq | example source: 謝謝 。\n",
            "2022-04-07 04:11:23 | INFO | hw5.seq2seq | example hypothesis: thank you .\n",
            "2022-04-07 04:11:23 | INFO | hw5.seq2seq | example reference: thank you .\n",
            "2022-04-07 04:11:23 | INFO | hw5.seq2seq | validation loss:\t2.7009\n",
            "2022-04-07 04:11:23 | INFO | hw5.seq2seq | BLEU = 17.14 57.8/27.7/15.1/8.6 (BP = 0.803 ratio = 0.820 hyp_len = 62435 ref_len = 76142)\n",
            "2022-04-07 04:11:25 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint9.pt\n",
            "2022-04-07 04:11:25 | INFO | hw5.seq2seq | end of epoch 9\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "531213249b9b4d4588819573e3b22352",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 10:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 04:21:33 | INFO | hw5.seq2seq | training loss: 2.5165\n",
            "2022-04-07 04:21:33 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba2cb40814db48a895c9ac272c169ef8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 04:22:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 04:22:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 04:22:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 04:22:02 | INFO | hw5.seq2seq | example source: 合作社的成員是自我選拔 。\n",
            "2022-04-07 04:22:02 | INFO | hw5.seq2seq | example hypothesis: the members of the collaborators are selfreplication .\n",
            "2022-04-07 04:22:02 | INFO | hw5.seq2seq | example reference: the slime mould collective membership is selfselecting .\n",
            "2022-04-07 04:22:02 | INFO | hw5.seq2seq | validation loss:\t2.6598\n",
            "2022-04-07 04:22:02 | INFO | hw5.seq2seq | BLEU = 18.97 56.1/27.2/15.0/8.6 (BP = 0.900 ratio = 0.905 hyp_len = 68911 ref_len = 76142)\n",
            "2022-04-07 04:22:04 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint10.pt\n",
            "2022-04-07 04:22:05 | INFO | hw5.seq2seq | end of epoch 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "088418058b3a4c0e8fc2ceda7a11a1b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 11:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 04:32:13 | INFO | hw5.seq2seq | training loss: 2.4681\n",
            "2022-04-07 04:32:13 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3558868ec0dc4e0bb03f17f1f015e444",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 04:32:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 04:32:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 04:32:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 04:32:42 | INFO | hw5.seq2seq | example source: 我製作了實體版的 「 笨鳥先飛 」 , 而它不會從應用程式商店下架 。\n",
            "2022-04-07 04:32:42 | INFO | hw5.seq2seq | example hypothesis: i made a physical version of \" birds fly first , \" and it doesn't fight from the app store .\n",
            "2022-04-07 04:32:42 | INFO | hw5.seq2seq | example reference: i made a physical version of flappy bird that could never be taken off the app store .\n",
            "2022-04-07 04:32:42 | INFO | hw5.seq2seq | validation loss:\t2.6403\n",
            "2022-04-07 04:32:42 | INFO | hw5.seq2seq | BLEU = 19.56 55.5/27.0/15.0/8.7 (BP = 0.929 ratio = 0.931 hyp_len = 70891 ref_len = 76142)\n",
            "2022-04-07 04:32:44 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint11.pt\n",
            "2022-04-07 04:32:46 | INFO | hw5.seq2seq | end of epoch 11\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "037c919cafa341ac8329fd42440e2c13",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 12:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 04:42:53 | INFO | hw5.seq2seq | training loss: 2.4261\n",
            "2022-04-07 04:42:53 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d32f462170104cd0863f0f0855d42266",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 04:43:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 04:43:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 04:43:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 04:43:22 | INFO | hw5.seq2seq | example source: 那樣的特質導致一種網路架構 , 這種架構前所未有 , 其後的其它數位網路也無法比擬 。\n",
            "2022-04-07 04:43:22 | INFO | hw5.seq2seq | example hypothesis: that trait has led to a network framework that has never existed before , and the rest of the digital network is impossible to simulate .\n",
            "2022-04-07 04:43:22 | INFO | hw5.seq2seq | example reference: that ethos led to a network architecture , a structure that was unlike other digital networks then or since .\n",
            "2022-04-07 04:43:22 | INFO | hw5.seq2seq | validation loss:\t2.6320\n",
            "2022-04-07 04:43:22 | INFO | hw5.seq2seq | BLEU = 19.82 56.2/27.6/15.4/9.0 (BP = 0.920 ratio = 0.923 hyp_len = 70308 ref_len = 76142)\n",
            "2022-04-07 04:43:24 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint12.pt\n",
            "2022-04-07 04:43:25 | INFO | hw5.seq2seq | end of epoch 12\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2849c229c6447d7b9df1ccf5dab4a31",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 13:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 04:53:33 | INFO | hw5.seq2seq | training loss: 2.3894\n",
            "2022-04-07 04:53:33 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8af9d6957dee447ebdbd26f821f513be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 04:54:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 04:54:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 04:54:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 04:54:03 | INFO | hw5.seq2seq | example source: 我是個運動員 。 那是我唯一懂得的事 , 也是唯一做過的事 。\n",
            "2022-04-07 04:54:03 | INFO | hw5.seq2seq | example hypothesis: i'm an athlete . that's the only thing i know , and the only thing i've ever done .\n",
            "2022-04-07 04:54:03 | INFO | hw5.seq2seq | example reference: i was an athlete . that's all i knew . that's all i'd done .\n",
            "2022-04-07 04:54:03 | INFO | hw5.seq2seq | validation loss:\t2.6254\n",
            "2022-04-07 04:54:03 | INFO | hw5.seq2seq | BLEU = 19.99 55.7/27.3/15.3/8.9 (BP = 0.937 ratio = 0.939 hyp_len = 71486 ref_len = 76142)\n",
            "2022-04-07 04:54:05 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint13.pt\n",
            "2022-04-07 04:54:07 | INFO | hw5.seq2seq | end of epoch 13\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8d13dddf0ef41e8a6348de50d045188",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 14:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 05:04:16 | INFO | hw5.seq2seq | training loss: 2.3555\n",
            "2022-04-07 05:04:16 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0236db5418c4753bea45c7dfc1aa5ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 05:04:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 05:04:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 05:04:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 05:04:45 | INFO | hw5.seq2seq | example source: 我可以想像一個未來 , 在這個未來中 , 我們會很高興看到學齡前兒童與螢幕互動 。\n",
            "2022-04-07 05:04:45 | INFO | hw5.seq2seq | example hypothesis: i can imagine a future where we're going to be happy to see preschool children interacting with screens .\n",
            "2022-04-07 05:04:45 | INFO | hw5.seq2seq | example reference: i can envision a future where we would be excited to see a preschooler interacting with a screen .\n",
            "2022-04-07 05:04:45 | INFO | hw5.seq2seq | validation loss:\t2.6300\n",
            "2022-04-07 05:04:45 | INFO | hw5.seq2seq | BLEU = 19.90 56.1/27.6/15.4/8.9 (BP = 0.926 ratio = 0.929 hyp_len = 70717 ref_len = 76142)\n",
            "2022-04-07 05:04:47 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint14.pt\n",
            "2022-04-07 05:04:47 | INFO | hw5.seq2seq | end of epoch 14\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c078d9e569dd4eec8bb6af572ad084ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 15:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 05:14:56 | INFO | hw5.seq2seq | training loss: 2.3266\n",
            "2022-04-07 05:14:56 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eded3923889647ad9b03da3393e52839",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 05:15:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 05:15:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 05:15:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 05:15:26 | INFO | hw5.seq2seq | example source: 裡面沒有鋪設好的道路 , 也沒有水泥的停車位地基 。 在拖車停車位之間也沒有圍籬 。\n",
            "2022-04-07 05:15:26 | INFO | hw5.seq2seq | example hypothesis: there's no roads , there's no cement parking ground , there's no fence between the trailer park .\n",
            "2022-04-07 05:15:26 | INFO | hw5.seq2seq | example reference: it didn't have any paved roads in it , it didn't have the concrete slabs , it didn't have fencing to portion off your trailer slot from other trailer slots .\n",
            "2022-04-07 05:15:26 | INFO | hw5.seq2seq | validation loss:\t2.6220\n",
            "2022-04-07 05:15:26 | INFO | hw5.seq2seq | BLEU = 19.62 56.2/27.4/15.3/8.9 (BP = 0.915 ratio = 0.919 hyp_len = 69962 ref_len = 76142)\n",
            "2022-04-07 05:15:28 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint15.pt\n",
            "2022-04-07 05:15:28 | INFO | hw5.seq2seq | end of epoch 15\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4cd700eba8e34c5494f07eb8db405efe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 16:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 05:25:37 | INFO | hw5.seq2seq | training loss: 2.2992\n",
            "2022-04-07 05:25:37 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "569f423fb0be47d4ab48e632e3f9bd5f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 05:26:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 05:26:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 05:26:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 05:26:06 | INFO | hw5.seq2seq | example source: 嬰兒不會説話 , 對吧 ? 」\n",
            "2022-04-07 05:26:06 | INFO | hw5.seq2seq | example hypothesis: babies don't talk , right ? \"\n",
            "2022-04-07 05:26:06 | INFO | hw5.seq2seq | example reference: infants can't talk , right ? \"\n",
            "2022-04-07 05:26:06 | INFO | hw5.seq2seq | validation loss:\t2.6248\n",
            "2022-04-07 05:26:06 | INFO | hw5.seq2seq | BLEU = 19.93 56.3/27.7/15.5/9.0 (BP = 0.923 ratio = 0.926 hyp_len = 70492 ref_len = 76142)\n",
            "2022-04-07 05:26:08 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint16.pt\n",
            "2022-04-07 05:26:08 | INFO | hw5.seq2seq | end of epoch 16\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "faf18b42983a472c9c1b19d3503356c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 17:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 05:36:18 | INFO | hw5.seq2seq | training loss: 2.2733\n",
            "2022-04-07 05:36:18 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72660e2a4a6747d79b0a1e14c67395e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 05:36:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 05:36:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 05:36:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 05:36:48 | INFO | hw5.seq2seq | example source: 而身為老師、妻子這自然也是我每天都面對的問題\n",
            "2022-04-07 05:36:48 | INFO | hw5.seq2seq | example hypothesis: and as a teacher , as a wife , that's what i'm dealing with every day .\n",
            "2022-04-07 05:36:48 | INFO | hw5.seq2seq | example reference: so as a teacher and as a spouse , this is , of course , a problem i confront every day .\n",
            "2022-04-07 05:36:48 | INFO | hw5.seq2seq | validation loss:\t2.6269\n",
            "2022-04-07 05:36:48 | INFO | hw5.seq2seq | BLEU = 19.85 54.8/26.7/14.8/8.6 (BP = 0.956 ratio = 0.957 hyp_len = 72850 ref_len = 76142)\n",
            "2022-04-07 05:36:50 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint17.pt\n",
            "2022-04-07 05:36:50 | INFO | hw5.seq2seq | end of epoch 17\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "411a578efacd4afd9d63b39bd8f8f793",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 18:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 05:46:57 | INFO | hw5.seq2seq | training loss: 2.2517\n",
            "2022-04-07 05:46:57 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "037f586218334b88b957546b15f1353b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 05:47:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 05:47:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 05:47:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 05:47:27 | INFO | hw5.seq2seq | example source: 後來我們真的這樣做了 , 那是好事 。\n",
            "2022-04-07 05:47:27 | INFO | hw5.seq2seq | example hypothesis: and then we did , and that's a good thing .\n",
            "2022-04-07 05:47:27 | INFO | hw5.seq2seq | example reference: so we did that . it was a good thing .\n",
            "2022-04-07 05:47:27 | INFO | hw5.seq2seq | validation loss:\t2.6264\n",
            "2022-04-07 05:47:27 | INFO | hw5.seq2seq | BLEU = 19.93 55.7/27.4/15.3/9.0 (BP = 0.932 ratio = 0.934 hyp_len = 71118 ref_len = 76142)\n",
            "2022-04-07 05:47:29 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint18.pt\n",
            "2022-04-07 05:47:29 | INFO | hw5.seq2seq | end of epoch 18\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ad530d9f04347469396fae07bbb42d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 19:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 05:57:37 | INFO | hw5.seq2seq | training loss: 2.2312\n",
            "2022-04-07 05:57:37 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c245079976034dd2aadb7f17807ba53b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 05:58:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 05:58:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 05:58:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 05:58:07 | INFO | hw5.seq2seq | example source: 他們選擇昂貴的檢測他們選擇為體弱的老人動手術\n",
            "2022-04-07 05:58:07 | INFO | hw5.seq2seq | example hypothesis: they chose expensive tests , and they chose to operate on weak old people .\n",
            "2022-04-07 05:58:07 | INFO | hw5.seq2seq | example reference: you choose an expensive lab test , you choose to operate on an old and frail patient .\n",
            "2022-04-07 05:58:07 | INFO | hw5.seq2seq | validation loss:\t2.6262\n",
            "2022-04-07 05:58:07 | INFO | hw5.seq2seq | BLEU = 20.28 55.2/27.1/15.2/8.9 (BP = 0.957 ratio = 0.958 hyp_len = 72907 ref_len = 76142)\n",
            "2022-04-07 05:58:09 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint19.pt\n",
            "2022-04-07 05:58:10 | INFO | hw5.seq2seq | end of epoch 19\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16a58cd4441c461283ae27790889887c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 20:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 06:08:19 | INFO | hw5.seq2seq | training loss: 2.2113\n",
            "2022-04-07 06:08:19 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "590a276f1c114274960f543558a3829e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 06:08:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 06:08:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 06:08:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 06:08:49 | INFO | hw5.seq2seq | example source: 我是說 , 這些事都在他眼下發生\n",
            "2022-04-07 06:08:49 | INFO | hw5.seq2seq | example hypothesis: i mean , all of this is happening under his eyes .\n",
            "2022-04-07 06:08:49 | INFO | hw5.seq2seq | example reference: i mean after all , this whole business happened on his watch .\n",
            "2022-04-07 06:08:49 | INFO | hw5.seq2seq | validation loss:\t2.6398\n",
            "2022-04-07 06:08:49 | INFO | hw5.seq2seq | BLEU = 19.72 55.8/27.3/15.2/8.8 (BP = 0.930 ratio = 0.932 hyp_len = 70982 ref_len = 76142)\n",
            "2022-04-07 06:08:50 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint20.pt\n",
            "2022-04-07 06:08:50 | INFO | hw5.seq2seq | end of epoch 20\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7cd8153eb084101ac5afb2d933ac83e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 21:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 06:19:00 | INFO | hw5.seq2seq | training loss: 2.1931\n",
            "2022-04-07 06:19:00 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e35e66c0c8d54b258b71499767f64ca5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 06:19:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 06:19:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 06:19:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 06:19:29 | INFO | hw5.seq2seq | example source: 有50%的車禍發生在十字路口\n",
            "2022-04-07 06:19:29 | INFO | hw5.seq2seq | example hypothesis: fifty percent of the crashes occur in crossroads .\n",
            "2022-04-07 06:19:29 | INFO | hw5.seq2seq | example reference: fifty percent of crashes happen at intersections .\n",
            "2022-04-07 06:19:29 | INFO | hw5.seq2seq | validation loss:\t2.6448\n",
            "2022-04-07 06:19:29 | INFO | hw5.seq2seq | BLEU = 19.83 56.2/27.5/15.3/8.8 (BP = 0.928 ratio = 0.931 hyp_len = 70856 ref_len = 76142)\n",
            "2022-04-07 06:19:31 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint21.pt\n",
            "2022-04-07 06:19:31 | INFO | hw5.seq2seq | end of epoch 21\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4dfab045362b41dc9bcf2ef7d3690643",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 22:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 06:29:38 | INFO | hw5.seq2seq | training loss: 2.1752\n",
            "2022-04-07 06:29:38 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b5a34c4941474bc8aae83be76f3b843d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 06:30:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 06:30:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 06:30:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 06:30:08 | INFO | hw5.seq2seq | example source: 我們要問自己這些問題 , 不論這是多麼的令人不快 。\n",
            "2022-04-07 06:30:08 | INFO | hw5.seq2seq | example hypothesis: we have to ask ourselves these questions , no matter how unpleasant it is .\n",
            "2022-04-07 06:30:08 | INFO | hw5.seq2seq | example reference: we have to ask ourselves these questions , however unpalatable .\n",
            "2022-04-07 06:30:08 | INFO | hw5.seq2seq | validation loss:\t2.6508\n",
            "2022-04-07 06:30:08 | INFO | hw5.seq2seq | BLEU = 19.63 56.5/27.6/15.4/9.0 (BP = 0.911 ratio = 0.915 hyp_len = 69653 ref_len = 76142)\n",
            "2022-04-07 06:30:09 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint22.pt\n",
            "2022-04-07 06:30:09 | INFO | hw5.seq2seq | end of epoch 22\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a4ee0f5226b48d89181586545516e43",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 23:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 06:40:17 | INFO | hw5.seq2seq | training loss: 2.1592\n",
            "2022-04-07 06:40:17 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6087279522fa46bf929f186f4e75cdad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 06:40:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 06:40:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 06:40:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 06:40:47 | INFO | hw5.seq2seq | example source: 有些會太靠近恆星 , 會被烤熟 , 有些則太遙遠 , 會被冰凍 。\n",
            "2022-04-07 06:40:47 | INFO | hw5.seq2seq | example hypothesis: some of them are too close to stars that they'll be took , and some of them too far will be frozen .\n",
            "2022-04-07 06:40:47 | INFO | hw5.seq2seq | example reference: some will be too close to a star and they'll fry , some will be too far away and they'll freeze .\n",
            "2022-04-07 06:40:47 | INFO | hw5.seq2seq | validation loss:\t2.6605\n",
            "2022-04-07 06:40:47 | INFO | hw5.seq2seq | BLEU = 19.66 56.3/27.5/15.3/8.8 (BP = 0.919 ratio = 0.923 hyp_len = 70245 ref_len = 76142)\n",
            "2022-04-07 06:40:48 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint23.pt\n",
            "2022-04-07 06:40:48 | INFO | hw5.seq2seq | end of epoch 23\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44577ad03b814c6c961cf4f6f803b49d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 24:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 06:50:57 | INFO | hw5.seq2seq | training loss: 2.1452\n",
            "2022-04-07 06:50:57 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67e8b5663c3a443f8fed2f432b436f57",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 06:51:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 06:51:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 06:51:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 06:51:27 | INFO | hw5.seq2seq | example source: 我認為大多數的人都不想死但我認為大多數的人都想要控制自己的死亡過程\n",
            "2022-04-07 06:51:27 | INFO | hw5.seq2seq | example hypothesis: i think most people don't want to die , but i think most people want to control their mortality .\n",
            "2022-04-07 06:51:27 | INFO | hw5.seq2seq | example reference: i think most people don't want to be dead , but i do think most people want to have some control over how their dying process proceeds .\n",
            "2022-04-07 06:51:27 | INFO | hw5.seq2seq | validation loss:\t2.6580\n",
            "2022-04-07 06:51:27 | INFO | hw5.seq2seq | BLEU = 19.99 56.2/27.6/15.5/9.1 (BP = 0.924 ratio = 0.927 hyp_len = 70558 ref_len = 76142)\n",
            "2022-04-07 06:51:28 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint24.pt\n",
            "2022-04-07 06:51:28 | INFO | hw5.seq2seq | end of epoch 24\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99cbc5c70cf840808c0891b1e46223d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 25:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 07:01:38 | INFO | hw5.seq2seq | training loss: 2.1303\n",
            "2022-04-07 07:01:38 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b58bc6cb38554254b3cfcb6ba9a8bb90",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 07:02:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 07:02:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 07:02:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 07:02:07 | INFO | hw5.seq2seq | example source: 目前我們正在土耳其建造一座更長一點的橋 , 我們也設計了義大利的墨西拿海峽大橋 , 正在等待開始動工的日子 , 天知道是何時 。\n",
            "2022-04-07 07:02:07 | INFO | hw5.seq2seq | example hypothesis: we're currently building a longer , longer bridge in turkey , and we're designing a canyon bridge in mexico , and we're waiting for the start of day to know when .\n",
            "2022-04-07 07:02:07 | INFO | hw5.seq2seq | example reference: we're currently working on one in turkey which is a bit longer , and we've designed the messina bridge in italy , which is just waiting to get started with construction one day , who knows when .\n",
            "2022-04-07 07:02:07 | INFO | hw5.seq2seq | validation loss:\t2.6645\n",
            "2022-04-07 07:02:07 | INFO | hw5.seq2seq | BLEU = 19.74 56.6/27.7/15.5/9.0 (BP = 0.913 ratio = 0.916 hyp_len = 69776 ref_len = 76142)\n",
            "2022-04-07 07:02:08 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint25.pt\n",
            "2022-04-07 07:02:08 | INFO | hw5.seq2seq | end of epoch 25\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2527f734f36443a85c11e123c615db1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 26:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 07:12:17 | INFO | hw5.seq2seq | training loss: 2.1175\n",
            "2022-04-07 07:12:17 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "225f92cd260e40f8ba587e0d29ad9c2d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 07:12:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 07:12:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 07:12:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 07:12:46 | INFO | hw5.seq2seq | example source: 國王在如雷掌聲中接過皇冠、權仗亨利·克里斯托夫登上離地20米的寶座\n",
            "2022-04-07 07:12:46 | INFO | hw5.seq2seq | example hypothesis: the king received the crown , henry christopher in the arms of the crown , 20 meters .\n",
            "2022-04-07 07:12:46 | INFO | hw5.seq2seq | example reference: after receiving his ornate crown and scepter , henry christophe ascended his throne , towering 20 meters in the air .\n",
            "2022-04-07 07:12:46 | INFO | hw5.seq2seq | validation loss:\t2.6730\n",
            "2022-04-07 07:12:46 | INFO | hw5.seq2seq | BLEU = 19.81 56.4/27.6/15.4/8.9 (BP = 0.922 ratio = 0.925 hyp_len = 70397 ref_len = 76142)\n",
            "2022-04-07 07:12:48 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint26.pt\n",
            "2022-04-07 07:12:48 | INFO | hw5.seq2seq | end of epoch 26\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f58ef18d67c4f06b0cfb2bce16a7557",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 27:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 07:22:57 | INFO | hw5.seq2seq | training loss: 2.1033\n",
            "2022-04-07 07:22:57 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d427a3a13ce2428dabdb96bcae0c84e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 07:23:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 07:23:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 07:23:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 07:23:27 | INFO | hw5.seq2seq | example source: 他們不知道那有多荒謬 , 而我知道 。\n",
            "2022-04-07 07:23:27 | INFO | hw5.seq2seq | example hypothesis: they didn't know how ridiculous it was , and i knew .\n",
            "2022-04-07 07:23:27 | INFO | hw5.seq2seq | example reference: now , they don't know how ridiculous that is , but i do .\n",
            "2022-04-07 07:23:27 | INFO | hw5.seq2seq | validation loss:\t2.6829\n",
            "2022-04-07 07:23:27 | INFO | hw5.seq2seq | BLEU = 19.56 56.3/27.4/15.3/8.9 (BP = 0.914 ratio = 0.917 hyp_len = 69843 ref_len = 76142)\n",
            "2022-04-07 07:23:28 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint27.pt\n",
            "2022-04-07 07:23:28 | INFO | hw5.seq2seq | end of epoch 27\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0574cf3f2fd1474b827837dd34fade1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 28:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 07:33:36 | INFO | hw5.seq2seq | training loss: 2.0921\n",
            "2022-04-07 07:33:36 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90a0039d19014610ab3f524ad66361e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 07:34:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 07:34:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 07:34:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 07:34:05 | INFO | hw5.seq2seq | example source: 這概念就是大家所知的 「 熱輻射 」 。\n",
            "2022-04-07 07:34:05 | INFO | hw5.seq2seq | example hypothesis: this is the idea that you know , thermal radiation .\n",
            "2022-04-07 07:34:05 | INFO | hw5.seq2seq | example reference: this is a concept known as thermal radiation .\n",
            "2022-04-07 07:34:05 | INFO | hw5.seq2seq | validation loss:\t2.6763\n",
            "2022-04-07 07:34:05 | INFO | hw5.seq2seq | BLEU = 19.69 55.8/27.2/15.1/8.7 (BP = 0.931 ratio = 0.934 hyp_len = 71083 ref_len = 76142)\n",
            "2022-04-07 07:34:07 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint28.pt\n",
            "2022-04-07 07:34:07 | INFO | hw5.seq2seq | end of epoch 28\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "954aadb8ad0c42d795fb53e9c5544b4f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 29:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 07:44:15 | INFO | hw5.seq2seq | training loss: 2.0801\n",
            "2022-04-07 07:44:15 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad3a89f8327d482ab6972c233c4f951d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 07:44:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 07:44:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 07:44:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 07:44:44 | INFO | hw5.seq2seq | example source: 老實說 , 我們其實可以提前完成這個企畫 , 但我想我們因為這些喝茶休憩時間多做了三周 。\n",
            "2022-04-07 07:44:44 | INFO | hw5.seq2seq | example hypothesis: and honestly , we can actually do this project ahead of time , but i think we've taken three more time open for this recession .\n",
            "2022-04-07 07:44:44 | INFO | hw5.seq2seq | example reference: and to be honest with you , we could have finished earlier , but i think it took us three weeks because of all those tea breaks .\n",
            "2022-04-07 07:44:44 | INFO | hw5.seq2seq | validation loss:\t2.6894\n",
            "2022-04-07 07:44:44 | INFO | hw5.seq2seq | BLEU = 19.82 56.2/27.5/15.4/9.0 (BP = 0.921 ratio = 0.924 hyp_len = 70369 ref_len = 76142)\n",
            "2022-04-07 07:44:46 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint29.pt\n",
            "2022-04-07 07:44:46 | INFO | hw5.seq2seq | end of epoch 29\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "457022aa08454001adeae3b55656c8f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 30:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 07:54:55 | INFO | hw5.seq2seq | training loss: 2.0702\n",
            "2022-04-07 07:54:55 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38dc7a75fd194b90a692d57fd2ec1abe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 07:55:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 07:55:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 07:55:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 07:55:24 | INFO | hw5.seq2seq | example source: 所以這是個好消息\n",
            "2022-04-07 07:55:24 | INFO | hw5.seq2seq | example hypothesis: so that's the good news .\n",
            "2022-04-07 07:55:24 | INFO | hw5.seq2seq | example reference: so that was good news .\n",
            "2022-04-07 07:55:24 | INFO | hw5.seq2seq | validation loss:\t2.6990\n",
            "2022-04-07 07:55:24 | INFO | hw5.seq2seq | BLEU = 19.59 55.7/27.0/15.0/8.7 (BP = 0.929 ratio = 0.931 hyp_len = 70916 ref_len = 76142)\n",
            "2022-04-07 07:55:25 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer-bt/checkpoint30.pt\n",
            "2022-04-07 07:55:26 | INFO | hw5.seq2seq | end of epoch 30\n"
          ]
        }
      ],
      "source": [
        "epoch_itr = load_data_iterator(task, \"train\", config.start_epoch, config.max_tokens, config.num_workers)\n",
        "try_load_checkpoint(model, optimizer, name=config.resume)\n",
        "while epoch_itr.next_epoch_idx <= config.max_epoch:\n",
        "    # train for one epoch\n",
        "    train_one_epoch(epoch_itr, model, task, criterion, optimizer, config.accum_steps)\n",
        "    stats = validate_and_save(model, task, criterion, optimizer, epoch=epoch_itr.epoch)\n",
        "    logger.info(\"end of epoch {}\".format(epoch_itr.epoch))    \n",
        "    epoch_itr = load_data_iterator(task, \"train\", epoch_itr.next_epoch_idx, config.max_tokens, config.num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyjRwllxPjtf"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "N70Gc6smPi1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(checkpoint_upper_bound=None, inputs=['./checkpoints/transformer-bt'], num_epoch_checkpoints=5, num_update_checkpoints=None, output='./checkpoints/transformer-bt/avg_last_5_checkpoint.pt')\n",
            "averaging checkpoints:  ['./checkpoints/transformer-bt/checkpoint30.pt', './checkpoints/transformer-bt/checkpoint29.pt', './checkpoints/transformer-bt/checkpoint28.pt', './checkpoints/transformer-bt/checkpoint27.pt', './checkpoints/transformer-bt/checkpoint26.pt']\n",
            "Finished writing averaged checkpoint to ./checkpoints/transformer-bt/avg_last_5_checkpoint.pt\n"
          ]
        }
      ],
      "source": [
        "# averaging a few checkpoints can have a similar effect to ensemble\n",
        "checkdir=config.savedir\n",
        "!python3 ./fairseq/scripts/average_checkpoints.py \\\n",
        "--inputs {checkdir} \\\n",
        "--num-epoch-checkpoints 5 \\\n",
        "--output {checkdir}/avg_last_5_checkpoint.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAGMiun8PnZy"
      },
      "source": [
        "## Confirm model weights used to generate submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "tvRdivVUPnsU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 07:55:28 | INFO | hw5.seq2seq | loaded checkpoint checkpoints/transformer-bt/avg_last_5_checkpoint.pt: step=unknown loss=2.698962688446045 bleu=19.59026836994649\n",
            "2022-04-07 07:55:28 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1af128368eb640c885e9f7197240ecd7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 07:55:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-04-07 07:55:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-04-07 07:55:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-04-07 07:55:57 | INFO | hw5.seq2seq | example source: 飛蚊可能不太明顯通常是如此\n",
            "2022-04-07 07:55:57 | INFO | hw5.seq2seq | example hypothesis: a flying mosquito might not be obvious , usually .\n",
            "2022-04-07 07:55:57 | INFO | hw5.seq2seq | example reference: floaters may be only barely distinguishable most of the time .\n",
            "2022-04-07 07:55:57 | INFO | hw5.seq2seq | validation loss:\t2.6542\n",
            "2022-04-07 07:55:57 | INFO | hw5.seq2seq | BLEU = 20.31 56.8/28.1/15.9/9.3 (BP = 0.922 ratio = 0.925 hyp_len = 70396 ref_len = 76142)\n"
          ]
        }
      ],
      "source": [
        "# checkpoint_last.pt : latest epoch\n",
        "# checkpoint_best.pt : highest validation bleu\n",
        "# avg_last_5_checkpoint.pt:　the average of last 5 epochs\n",
        "try_load_checkpoint(model, name=\"avg_last_5_checkpoint.pt\")\n",
        "validate(model, task, criterion, log_to_wandb=False)\n",
        "None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioAIflXpPsxt"
      },
      "source": [
        "## Generate Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "oYMxA8FlPtIq"
      },
      "outputs": [],
      "source": [
        "def generate_prediction(model, task, split=\"test\", outfile=\"./prediction.txt\"):    \n",
        "    task.load_dataset(split=split, epoch=1)\n",
        "    itr = load_data_iterator(task, split, 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
        "    \n",
        "    idxs = []\n",
        "    hyps = []\n",
        "\n",
        "    model.eval()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"prediction\")\n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(progress):\n",
        "            # validation loss\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "\n",
        "            # do inference\n",
        "            s, h, r = inference_step(sample, model)\n",
        "            \n",
        "            hyps.extend(h)\n",
        "            idxs.extend(list(sample['id']))\n",
        "            \n",
        "    # sort based on the order before preprocess\n",
        "    hyps = [x for _,x in sorted(zip(idxs,hyps))]\n",
        "    \n",
        "    with open(outfile, \"w\") as f:\n",
        "        for h in hyps:\n",
        "            f.write(h+\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z0cJE-wPzaU"
      },
      "source": [
        "# Back-translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-7uPJ2CP0sm"
      },
      "source": [
        "## Train a backward translation model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppGHjg2ZP3sV"
      },
      "source": [
        "1. Switch the source_lang and target_lang in **config** \n",
        "2. Change the savedir in **config** (eg. \"./checkpoints/transformer-back\")\n",
        "3. Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waTGz29UP6WI"
      },
      "source": [
        "## Generate synthetic data with backward model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIeTsPexP8FL"
      },
      "source": [
        "### Download monolingual data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "i7N4QlsbP8fh"
      },
      "outputs": [],
      "source": [
        "mono_dataset_name = 'mono'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "396saD9-QBPY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ted_zh_corpus.deduped.gz is exist, skip downloading\n"
          ]
        }
      ],
      "source": [
        "mono_prefix = Path(data_dir).absolute() / mono_dataset_name\n",
        "mono_prefix.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "urls = (\n",
        "    \"https://github.com/yuhsinchan/ML2022-HW5Dataset/releases/download/v1.0.2/ted_zh_corpus.deduped.gz\",\n",
        ")\n",
        "file_names = (\n",
        "    'ted_zh_corpus.deduped.gz',\n",
        ")\n",
        "\n",
        "for u, f in zip(urls, file_names):\n",
        "    path = mono_prefix/f\n",
        "    if not path.exists():\n",
        "        !wget {u} -O {path}\n",
        "    else:\n",
        "        print(f'{f} is exist, skip downloading')\n",
        "    if path.suffix == \".tgz\":\n",
        "        !tar -xvf {path} -C {prefix}\n",
        "    elif path.suffix == \".zip\":\n",
        "        !unzip -o {path} -d {prefix}\n",
        "    elif path.suffix == \".gz\":\n",
        "        !gzip -fkd {path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOVQRHzGQU4-"
      },
      "source": [
        "### TODO: clean corpus\n",
        "\n",
        "1. remove sentences that are too long or too short\n",
        "2. unify punctuation\n",
        "\n",
        "hint: you can use clean_s() defined above to do this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_mono_corpus(mono_prefix, l1, l2, max_len=1000, min_len=1):\n",
        "    if Path(f'{mono_prefix}/ted_zh_corpus.deduped.clean.{l1}').exists() and Path(f'{mono_prefix}/ted_zh_corpus.deduped.clean.{l2}').exists():\n",
        "        print(f'{mono_prefix}/ted_zh_corpus.deduped.clean.{l1} & {l2} exists. skipping clean.')\n",
        "        return\n",
        "    with open(f'{mono_prefix}/ted_zh_corpus.deduped', 'r') as l1_in_f:\n",
        "        with open(f'{mono_prefix}/ted_zh_corpus.deduped.clean.{l1}', 'w') as l1_out_f:\n",
        "            with open(f'{mono_prefix}/ted_zh_corpus.deduped.clean.{l2}', 'w') as l2_out_f:\n",
        "                for s1 in l1_in_f:\n",
        "                    s1 = s1.strip()\n",
        "                    s1 = clean_s(s1, l1)\n",
        "                    s1_len = len_s(s1, l1)\n",
        "                    if min_len > 0: # remove short sentence\n",
        "                        if s1_len < min_len:\n",
        "                            continue\n",
        "                    if max_len > 0: # remove long sentence\n",
        "                        if s1_len > max_len:\n",
        "                            continue\n",
        "                    print(s1, file=l1_out_f)\n",
        "                    print('.', file=l2_out_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/weiweichi/weichi/HW5/DATA/rawdata/mono\n",
            "/home/weiweichi/weichi/HW5/DATA/rawdata/mono/ted_zh_corpus.deduped.clean.zh & en exists. skipping clean.\n"
          ]
        }
      ],
      "source": [
        "print(mono_prefix)\n",
        "clean_mono_corpus(mono_prefix, 'zh','en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "非常謝謝你 , 克里斯 。 能有這個機會第二度踏上這個演講台\n",
            "真是一大榮幸 。 我非常感激 。\n",
            "這個研討會給我留下了極為深刻的印象 , 我想感謝大家對我之前演講的好評 。\n",
            "我是由衷的想這麼說 , 有部份原因是因為我真的有需要 !\n",
            "請你們設身處地為我想一想 !\n",
            "Thank you so much , Chris .\n",
            "And it's truly a great honor to have the opportunity to come to this stage twice ; I'm extremely grateful .\n",
            "I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
            "And I say that sincerely , partly because I need that .\n",
            "Put yourselves in my position .\n"
          ]
        }
      ],
      "source": [
        "!head {data_prefix+'.clean.'+'zh'} -n 5\n",
        "!head {data_prefix+'.clean.'+'en'} -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jegH0bvMQVmR"
      },
      "source": [
        "### TODO: Subword Units\n",
        "\n",
        "Use the spm model of the backward model to tokenize the data into subword units\n",
        "\n",
        "hint: spm model is located at DATA/raw-data/\\[dataset\\]/spm\\[vocab_num\\].model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "vqgR4uUMQZGY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/weiweichi/weichi/HW5/DATA/rawdata/mono/mono.tok.zh exists. skipping spm_encode.\n",
            "/home/weiweichi/weichi/HW5/DATA/rawdata/mono/mono.tok.en exists. skipping spm_encode.\n"
          ]
        }
      ],
      "source": [
        "for lang in ['zh', 'en']:\n",
        "    out_path = mono_prefix/f'mono.tok.{lang}'\n",
        "    if out_path.exists():\n",
        "        print(f\"{out_path} exists. skipping spm_encode.\")\n",
        "    else:\n",
        "        with open(mono_prefix/f'mono.tok.{lang}', 'w') as out_f:\n",
        "            with open(mono_prefix/f'ted_zh_corpus.deduped.clean.{lang}', 'r') as in_f:\n",
        "                for line in in_f:\n",
        "                    line = line.strip()\n",
        "                    tok = spm_model.encode(line, out_type=str)\n",
        "                    print(' '.join(tok), file=out_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a65glBVXQZiE"
      },
      "source": [
        "### Binarize\n",
        "\n",
        "use fairseq to binarize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "b803qA5aQaEu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATA/data-bin/mono exists, will not overwrite!\n"
          ]
        }
      ],
      "source": [
        "binpath = Path('./DATA/data-bin', mono_dataset_name)\n",
        "src_dict_file = './DATA/data-bin/ted2020/dict.en.txt'\n",
        "tgt_dict_file = src_dict_file\n",
        "monopref = str(mono_prefix/\"mono.tok\") # whatever filepath you get after applying subword tokenization\n",
        "if binpath.exists():\n",
        "    print(binpath, \"exists, will not overwrite!\")\n",
        "else:\n",
        "    !python3 -m fairseq_cli.preprocess\\\n",
        "        --source-lang 'zh'\\\n",
        "        --target-lang 'en'\\\n",
        "        --trainpref {monopref}\\\n",
        "        --destdir {binpath}\\\n",
        "        --srcdict {src_dict_file}\\\n",
        "        --tgtdict {tgt_dict_file}\\\n",
        "        --workers 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smA0JraEQdxz"
      },
      "source": [
        "### TODO: Generate synthetic data with backward model\n",
        "\n",
        "Add binarized monolingual data to the original data directory, and name it with \"split_name\"\n",
        "\n",
        "ex. ./DATA/data-bin/ted2020/\\[split_name\\].zh-en.\\[\"en\", \"zh\"\\].\\[\"bin\", \"idx\"\\]\n",
        "\n",
        "then you can use 'generate_prediction(model, task, split=\"split_name\")' to generate translation prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "jvaOVHeoQfkB"
      },
      "outputs": [],
      "source": [
        "# Add binarized monolingual data to the original data directory, and name it with \"split_name\"\n",
        "# ex. ./DATA/data-bin/ted2020/\\[split_name\\].zh-en.\\[\"en\", \"zh\"\\].\\[\"bin\", \"idx\"\\]\n",
        "!cp ./DATA/data-bin/mono/train.zh-en.zh.bin ./DATA/data-bin/ted2020/mono.zh-en.zh.bin\n",
        "!cp ./DATA/data-bin/mono/train.zh-en.zh.idx ./DATA/data-bin/ted2020/mono.zh-en.zh.idx\n",
        "!cp ./DATA/data-bin/mono/train.zh-en.en.bin ./DATA/data-bin/ted2020/mono.zh-en.en.bin\n",
        "!cp ./DATA/data-bin/mono/train.zh-en.en.idx ./DATA/data-bin/ted2020/mono.zh-en.en.idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "fFEkxPu-Qhlc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 07:56:04 | INFO | fairseq.data.data_utils | loaded 781,713 examples from: ./DATA/data-bin/ted2020/mono.zh-en.zh\n",
            "2022-04-07 07:56:04 | INFO | fairseq.data.data_utils | loaded 781,713 examples from: ./DATA/data-bin/ted2020/mono.zh-en.en\n",
            "2022-04-07 07:56:04 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 mono zh-en 781713 examples\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "001971295e4a48848bcb40a53196c85a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "prediction:   0%|          | 0/1715 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# hint: do prediction on split='mono' to create prediction_file\n",
        "generate_prediction(model, task, split=\"mono\", outfile=\"./DATA/rawdata/mono/mono_prediction.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "in the mid16th century , italians were fascinated by a kind of male singer , broad range , including the pitch of the adults impossible to achieve by men .\n",
            "but this gift comes at a very high cost .\n",
            "to prevent them from turning voices , these singers were castrated before puberty to stop the hormonal changes in regard to lower their voice wires .\n",
            "it's called castrati , and their light , angelic voices are very wellknown throughout europe , until this cruel procedure was banned in the 19th century .\n",
            "although the vocal growth of stopping the vocal cords can produce an extraordinary range of voices , naturally developing voices can be tremendous .\n"
          ]
        }
      ],
      "source": [
        "!head {'./DATA/rawdata/mono/mono_prediction.txt'} -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn4XeawpQjLk"
      },
      "source": [
        "### TODO: Create new dataset\n",
        "\n",
        "1. Combine the prediction data with monolingual data\n",
        "2. Use the original spm model to tokenize data into Subword Units\n",
        "3. Binarize data with fairseq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "3R35JTaTQjkm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATA/data-bin/synthetic exists, will not overwrite!\n"
          ]
        }
      ],
      "source": [
        "# Combine prediction_file (.en) and mono.zh (.zh) into a new dataset.\n",
        "# \n",
        "# hint: tokenize prediction_file with the spm model\n",
        "# spm_model.encode(line, out_type=str)\n",
        "# output: ./DATA/rawdata/mono/mono.tok.en & mono.tok.zh\n",
        "#\n",
        "# hint: use fairseq to binarize these two files again\n",
        "binpath = Path('./DATA/data-bin/synthetic')\n",
        "src_dict_file = './DATA/data-bin/ted2020/dict.en.txt'\n",
        "tgt_dict_file = src_dict_file\n",
        "monopref = './DATA/rawdata/mono/mono.tok' # or whatever path after applying subword tokenization, w/o the suffix (.zh/.en)\n",
        "if binpath.exists():\n",
        "    print(binpath, \"exists, will not overwrite!\")\n",
        "else:\n",
        "    !python3 -m fairseq_cli.preprocess\\\n",
        "        --source-lang 'zh'\\\n",
        "        --target-lang 'en'\\\n",
        "        --trainpref {monopref}\\\n",
        "        --destdir {binpath}\\\n",
        "        --srcdict {src_dict_file}\\\n",
        "        --tgtdict {tgt_dict_file}\\\n",
        "        --workers 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "MSkse1tyQnsR"
      },
      "outputs": [],
      "source": [
        "# create a new dataset from all the files prepared above\n",
        "!cp -r ./DATA/data-bin/ted2020/ ./DATA/data-bin/ted2020_with_mono/\n",
        "\n",
        "!cp ./DATA/data-bin/synthetic/train.zh-en.zh.bin ./DATA/data-bin/ted2020_with_mono/train1.en-zh.zh.bin\n",
        "!cp ./DATA/data-bin/synthetic/train.zh-en.zh.idx ./DATA/data-bin/ted2020_with_mono/train1.en-zh.zh.idx\n",
        "!cp ./DATA/data-bin/synthetic/train.zh-en.en.bin ./DATA/data-bin/ted2020_with_mono/train1.en-zh.en.bin\n",
        "!cp ./DATA/data-bin/synthetic/train.zh-en.en.idx ./DATA/data-bin/ted2020_with_mono/train1.en-zh.en.idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVdxVGO3QrSs"
      },
      "source": [
        "Created new dataset \"ted2020_with_mono\"\n",
        "\n",
        "1. Change the datadir in **config** (\"./DATA/data-bin/ted2020_with_mono\")\n",
        "2. Switch back the source_lang and target_lang in **config** (\"en\", \"zh\")\n",
        "2. Change the savedir in **config** (eg. \"./checkpoints/transformer-bt\")\n",
        "3. Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CZU2beUQtl3"
      },
      "source": [
        "1. <a name=ott2019fairseq></a>Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., ... & Auli, M. (2019, June). fairseq: A Fast, Extensible Toolkit for Sequence Modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations) (pp. 48-53).\n",
        "2. <a name=vaswani2017></a>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017, December). Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (pp. 6000-6010).\n",
        "3. <a name=reimers-2020-multilingual-sentence-bert></a>Reimers, N., & Gurevych, I. (2020, November). Making Monolingual Sentence Embeddings Multilingual Using Knowledge Distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 4512-4525).\n",
        "4. <a name=tiedemann2012parallel></a>Tiedemann, J. (2012, May). Parallel Data, Tools and Interfaces in OPUS. In Lrec (Vol. 2012, pp. 2214-2218).\n",
        "5. <a name=kudo-richardson-2018-sentencepiece></a>Kudo, T., & Richardson, J. (2018, November). SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (pp. 66-71).\n",
        "6. <a name=sennrich-etal-2016-improving></a>Sennrich, R., Haddow, B., & Birch, A. (2016, August). Improving Neural Machine Translation Models with Monolingual Data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 86-96).\n",
        "7. <a name=edunov-etal-2018-understanding></a>Edunov, S., Ott, M., Auli, M., & Grangier, D. (2018). Understanding Back-Translation at Scale. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (pp. 489-500).\n",
        "8. https://github.com/ajinkyakulkarni14/TED-Multilingual-Parallel-Corpus\n",
        "9. https://ithelp.ithome.com.tw/articles/10233122\n",
        "10. https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "11. https://colab.research.google.com/github/ga642381/ML2021-Spring/blob/main/HW05/HW05.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "Rrfm6iLJQ0tS"
      },
      "outputs": [],
      "source": [
        "config = Namespace(\n",
        "    datadir = \"./DATA/data-bin/ted2020_with_mono\",\n",
        "    savedir = \"./checkpoints/transformer\",\n",
        "    source_lang = \"en\",\n",
        "    target_lang = \"zh\",\n",
        "    \n",
        "    # cpu threads when fetching & processing data.\n",
        "    num_workers=2,  \n",
        "    # batch size in terms of tokens. gradient accumulation increases the effective batchsize.\n",
        "    max_tokens=3072,\n",
        "    accum_steps=4,\n",
        "    \n",
        "    # the lr s calculated from Noam lr scheduler. you can tune the maximum lr by this factor.\n",
        "    lr_factor=2.,\n",
        "    lr_warmup=4000,\n",
        "    \n",
        "    # clipping gradient norm helps alleviate gradient exploding\n",
        "    clip_norm=1.0,\n",
        "    \n",
        "    # maximum epochs for training\n",
        "    max_epoch=35,\n",
        "    start_epoch=1,\n",
        "    \n",
        "    # beam size for beam search\n",
        "    beam=5, \n",
        "    # generate sequences of maximum length ax + b, where x is the source length\n",
        "    max_len_a=1.2, \n",
        "    max_len_b=10, \n",
        "    # when decoding, post process sentence by removing sentencepiece symbols and jieba tokenization.\n",
        "    post_process = \"sentencepiece\",\n",
        "    \n",
        "    # checkpoints\n",
        "    keep_last_epochs=5,\n",
        "    resume=None, # if resume from checkpoint name (under config.savedir)\n",
        "    \n",
        "    # logging\n",
        "    use_wandb=False,\n",
        ")\n",
        "if config.use_wandb:\n",
        "    import wandb\n",
        "    wandb.init(project=proj, name=Path(config.savedir).stem, config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 08:56:17 | INFO | fairseq.tasks.translation | [en] dictionary: 7992 types\n",
            "2022-04-07 08:56:17 | INFO | fairseq.tasks.translation | [zh] dictionary: 7992 types\n"
          ]
        }
      ],
      "source": [
        "task_cfg = TranslationConfig(\n",
        "    data=config.datadir,\n",
        "    source_lang=config.source_lang,\n",
        "    target_lang=config.target_lang,\n",
        "    train_subset=\"train\",\n",
        "    required_seq_len_multiple=8,\n",
        "    dataset_impl=\"mmap\",\n",
        "    upsample_primary=1,\n",
        ")\n",
        "task = TranslationTask.setup_task(task_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 08:56:17 | INFO | hw5.seq2seq | loading data for epoch 1\n",
            "2022-04-07 08:56:17 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./DATA/data-bin/ted2020_with_mono/train.en-zh.en\n",
            "2022-04-07 08:56:17 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./DATA/data-bin/ted2020_with_mono/train.en-zh.zh\n",
            "2022-04-07 08:56:17 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020_with_mono train en-zh 390041 examples\n",
            "2022-04-07 08:56:17 | INFO | fairseq.data.data_utils | loaded 781,713 examples from: ./DATA/data-bin/ted2020_with_mono/train1.en-zh.en\n",
            "2022-04-07 08:56:17 | INFO | fairseq.data.data_utils | loaded 781,713 examples from: ./DATA/data-bin/ted2020_with_mono/train1.en-zh.zh\n",
            "2022-04-07 08:56:17 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020_with_mono train1 en-zh 781713 examples\n",
            "2022-04-07 08:56:17 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./DATA/data-bin/ted2020_with_mono/valid.en-zh.en\n",
            "2022-04-07 08:56:17 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./DATA/data-bin/ted2020_with_mono/valid.en-zh.zh\n",
            "2022-04-07 08:56:17 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020_with_mono valid en-zh 3939 examples\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"loading data for epoch 1\")\n",
        "task.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\n",
        "task.load_dataset(split=\"valid\", epoch=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 1,\n",
            " 'source': tensor([  35,   22,  440,   27,    5,  850,  261,  172,    6,   29,  800,    6,\n",
            "         184,  164,   99, 1178,  374,  804,  257,   41,    5,  313,  868,   42,\n",
            "          27,   61, 1836,   18,  184, 1147, 1404,   21,    6,    6,  320,    7,\n",
            "           2]),\n",
            " 'target': tensor([  53, 3134, 3107,   34,  408, 1223, 3895,  670,    9, 1038,  498,  148,\n",
            "        1689,  364,  129, 1050, 1103,   34,  408,  158,  607, 3268,  306,    9,\n",
            "        1105,    2])}\n",
            "('Source: so we bred flies whose brains were more or less randomly peppered '\n",
            " 'with cells that were light addressable .')\n",
            "'Target: 我們培植了一些果蠅它們的腦部被隨機地安置了一些可以光驅動的細胞'\n"
          ]
        }
      ],
      "source": [
        "sample = task.dataset(\"valid\")[1]\n",
        "pprint.pprint(sample)\n",
        "pprint.pprint(\n",
        "    \"Source: \" + \\\n",
        "    task.source_dictionary.string(\n",
        "        sample['source'],\n",
        "        config.post_process,\n",
        "    )\n",
        ")\n",
        "pprint.pprint(\n",
        "    \"Target: \" + \\\n",
        "    task.target_dictionary.string(\n",
        "        sample['target'],\n",
        "        config.post_process,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 08:56:17 | WARNING | fairseq.tasks.fairseq_task | 2,565 samples have invalid sizes and will be skipped, max_positions=(20, 20), first few sample ids=[1643, 1630, 3670, 1642, 413, 2460, 3921, 681, 2996, 302]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'id': tensor([2710, 1186]),\n",
              " 'nsentences': 2,\n",
              " 'ntokens': 18,\n",
              " 'net_input': {'src_tokens': tensor([[  19,  269,   80,   31,   88,  617,    7,    2],\n",
              "          [  19,  185,   56,   18, 1119, 2760,    7,    2]]),\n",
              "  'src_lengths': tensor([8, 8]),\n",
              "  'prev_output_tokens': tensor([[   2,   40,  158, 1028,  336,  125,  444,  162,   10,    1,    1,    1,\n",
              "              1,    1,    1,    1],\n",
              "          [   2, 2375, 1979,  309, 2515,    9,  120,  782,   10,    1,    1,    1,\n",
              "              1,    1,    1,    1]])},\n",
              " 'target': tensor([[  40,  158, 1028,  336,  125,  444,  162,   10,    2,    1,    1,    1,\n",
              "             1,    1,    1,    1],\n",
              "         [2375, 1979,  309, 2515,    9,  120,  782,   10,    2,    1,    1,    1,\n",
              "             1,    1,    1,    1]])}"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "demo_epoch_obj = load_data_iterator(task, \"valid\", epoch=1, max_tokens=20, num_workers=1, cached=False)\n",
        "demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True)\n",
        "sample = next(demo_iter)\n",
        "sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "arch_args = Namespace(\n",
        "    encoder_embed_dim=1024,\n",
        "    encoder_ffn_embed_dim=3072,\n",
        "    encoder_layers=6,\n",
        "    decoder_embed_dim=1024,\n",
        "    decoder_ffn_embed_dim=3072,\n",
        "    decoder_layers=6,\n",
        "    share_decoder_input_output_embed=True,\n",
        "    dropout=0.3,\n",
        ")\n",
        "\n",
        "def add_transformer_args(args):\n",
        "    args.encoder_attention_heads=16\n",
        "    args.encoder_normalize_before=True\n",
        "    \n",
        "    args.decoder_attention_heads=16\n",
        "    args.decoder_normalize_before=True\n",
        "    \n",
        "    args.activation_fn=\"relu\"\n",
        "    args.max_source_positions=1024\n",
        "    args.max_target_positions=1024\n",
        "    \n",
        "    # patches on default parameters for Transformer (those not set above)\n",
        "    from fairseq.models.transformer import base_architecture\n",
        "    base_architecture(arch_args)\n",
        "\n",
        "add_transformer_args(arch_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = build_model(arch_args, task)\n",
        "criterion = LabelSmoothedCrossEntropyCriterion(\n",
        "    smoothing=0.1,\n",
        "    ignore_index=task.target_dictionary.pad(),\n",
        ")\n",
        "optimizer = NoamOpt(\n",
        "    model_size=arch_args.encoder_embed_dim, \n",
        "    factor=config.lr_factor, \n",
        "    warmup=config.lr_warmup, \n",
        "    optimizer=torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001)\n",
        ")\n",
        "sequence_generator = task.build_generator([model], config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = model.to(device=device)\n",
        "criterion = criterion.to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 08:56:20 | INFO | hw5.seq2seq | task: TranslationTask\n",
            "2022-04-07 08:56:20 | INFO | hw5.seq2seq | encoder: TransformerEncoder\n",
            "2022-04-07 08:56:20 | INFO | hw5.seq2seq | decoder: TransformerDecoder\n",
            "2022-04-07 08:56:20 | INFO | hw5.seq2seq | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2022-04-07 08:56:20 | INFO | hw5.seq2seq | optimizer: NoamOpt\n",
            "2022-04-07 08:56:20 | INFO | hw5.seq2seq | num. model params: 167,550,976 (num. trained: 167,550,976)\n",
            "2022-04-07 08:56:20 | INFO | hw5.seq2seq | max tokens per batch = 3072, accumulate steps = 4\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"task: {}\".format(task.__class__.__name__))\n",
        "logger.info(\"encoder: {}\".format(model.encoder.__class__.__name__))\n",
        "logger.info(\"decoder: {}\".format(model.decoder.__class__.__name__))\n",
        "logger.info(\"criterion: {}\".format(criterion.__class__.__name__))\n",
        "logger.info(\"optimizer: {}\".format(optimizer.__class__.__name__))\n",
        "logger.info(\n",
        "    \"num. model params: {:,} (num. trained: {:,})\".format(\n",
        "        sum(p.numel() for p in model.parameters()),\n",
        "        sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
        "    )\n",
        ")\n",
        "logger.info(f\"max tokens per batch = {config.max_tokens}, accumulate steps = {config.accum_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 08:56:20 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[326738]\n",
            "2022-04-07 08:56:20 | INFO | hw5.seq2seq | no checkpoints found at checkpoints/transformer/checkpoint_last.pt!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7683ea6ecd446aab8231133cc055975",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 1:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 09:57:13 | INFO | hw5.seq2seq | training loss: 5.6832\n",
            "2022-04-07 09:57:13 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c539a5a59df4ee884edde68992015df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 09:58:28 | INFO | hw5.seq2seq | example source: my name is joseph , a member of parliament in kenya .\n",
            "2022-04-07 09:58:28 | INFO | hw5.seq2seq | example hypothesis: 我的名字是 , 阿拉伯的阿拉伯人 。\n",
            "2022-04-07 09:58:28 | INFO | hw5.seq2seq | example reference: 我的名字是喬瑟夫 , 我是肯亞的國會議員 。\n",
            "2022-04-07 09:58:28 | INFO | hw5.seq2seq | validation loss:\t4.3661\n",
            "2022-04-07 09:58:28 | INFO | hw5.seq2seq | BLEU = 9.08 31.6/12.8/5.9/2.9 (BP = 1.000 ratio = 1.059 hyp_len = 118214 ref_len = 111605)\n",
            "2022-04-07 09:58:29 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint1.pt\n",
            "2022-04-07 09:58:29 | INFO | hw5.seq2seq | end of epoch 1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dca74dc8134547a6b97818203b4f5745",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 2:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 10:59:18 | INFO | hw5.seq2seq | training loss: 4.6815\n",
            "2022-04-07 10:59:18 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5166f05c8fd2434d9915816e5b7151ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 11:00:10 | INFO | hw5.seq2seq | example source: in all this world of instant gratification and 24/7 , ondemand results , scientists require persistence , vision and patience to rise above all that .\n",
            "2022-04-07 11:00:10 | INFO | hw5.seq2seq | example hypothesis: 在這24個國家 , 科學家需要維持永續發展的願景 , 來解決這些問題的願景 。\n",
            "2022-04-07 11:00:10 | INFO | hw5.seq2seq | example reference: 這個世界熱愛即時滿足 , 崇拜急功近利 , 但科學家需要毅力、視野和耐心 , 來讓這些成為現實 。\n",
            "2022-04-07 11:00:10 | INFO | hw5.seq2seq | validation loss:\t3.9413\n",
            "2022-04-07 11:00:10 | INFO | hw5.seq2seq | BLEU = 12.57 49.4/23.2/11.9/6.5 (BP = 0.728 ratio = 0.759 hyp_len = 84687 ref_len = 111605)\n",
            "2022-04-07 11:00:17 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint2.pt\n",
            "2022-04-07 11:00:17 | INFO | hw5.seq2seq | end of epoch 2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09cad5ef0c774c289825f2eb44785564",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 3:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 12:01:03 | INFO | hw5.seq2seq | training loss: 4.3963\n",
            "2022-04-07 12:01:03 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91600baefba34107af67020bd498458c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 12:02:00 | INFO | hw5.seq2seq | example source: let's look at a specific video so you can see how it works .\n",
            "2022-04-07 12:02:00 | INFO | hw5.seq2seq | example hypothesis: 我們來看一個特定的影片 , 來看看它是怎麼運作的 。\n",
            "2022-04-07 12:02:00 | INFO | hw5.seq2seq | example reference: 讓我們以一個影片為例 , 來了解這是如何運作的 。\n",
            "2022-04-07 12:02:00 | INFO | hw5.seq2seq | validation loss:\t3.5914\n",
            "2022-04-07 12:02:00 | INFO | hw5.seq2seq | BLEU = 17.11 51.5/26.0/14.2/8.2 (BP = 0.861 ratio = 0.870 hyp_len = 97073 ref_len = 111605)\n",
            "2022-04-07 12:02:07 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint3.pt\n",
            "2022-04-07 12:02:07 | INFO | hw5.seq2seq | end of epoch 3\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7d3f9e0bfb04b23b3b73b0b570ade0d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 4:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 13:02:51 | INFO | hw5.seq2seq | training loss: 4.1924\n",
            "2022-04-07 13:02:51 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "765eea1ba47f4efb90af992221b6349f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 13:03:48 | INFO | hw5.seq2seq | example source: i said , \" well , do it . \"\n",
            "2022-04-07 13:03:48 | INFO | hw5.seq2seq | example hypothesis: 我說: 「 嗯 , 做得好 。 」\n",
            "2022-04-07 13:03:48 | INFO | hw5.seq2seq | example reference: 我說: \" 解題阿 \"\n",
            "2022-04-07 13:03:48 | INFO | hw5.seq2seq | validation loss:\t3.3993\n",
            "2022-04-07 13:03:48 | INFO | hw5.seq2seq | BLEU = 19.88 52.2/27.3/15.4/9.2 (BP = 0.939 ratio = 0.940 hyp_len = 104951 ref_len = 111605)\n",
            "2022-04-07 13:03:55 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint4.pt\n",
            "2022-04-07 13:03:55 | INFO | hw5.seq2seq | end of epoch 4\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09fd1fad988c4512b5bf8947c3db8b7e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 5:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 14:04:41 | INFO | hw5.seq2seq | training loss: 4.0631\n",
            "2022-04-07 14:04:41 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3e63e5ba25c48628399f0c8ee7e3057",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 14:05:41 | INFO | hw5.seq2seq | example source: when i asked people why they were digging their houses from the ground , they simply replied that they are poor wheat and apple farmers who didn't have the money to buy materials , and this digging out was their most logical form of living .\n",
            "2022-04-07 14:05:41 | INFO | hw5.seq2seq | example hypothesis: 當我問人們為什麼他們會在地上挖房子 , 他們只是回覆說他們是貧窮的 , 蘋果的農夫沒有錢買房子 , 而這就是他們生活中最合理的形式 。\n",
            "2022-04-07 14:05:41 | INFO | hw5.seq2seq | example reference: 我問這些居民為什麼要向下挖房子 , 他們只說了因為他們是種麥和蘋果的窮農夫 , 沒有錢買材料 , 這樣挖洞是最符合常理的生活方式 。\n",
            "2022-04-07 14:05:41 | INFO | hw5.seq2seq | validation loss:\t3.2903\n",
            "2022-04-07 14:05:41 | INFO | hw5.seq2seq | BLEU = 21.33 53.1/28.5/16.4/10.0 (BP = 0.955 ratio = 0.956 hyp_len = 106740 ref_len = 111605)\n",
            "2022-04-07 14:05:48 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint5.pt\n",
            "2022-04-07 14:05:49 | INFO | hw5.seq2seq | end of epoch 5\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "adac3b1f0da34d9fac6029d03329b0b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 6:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 15:06:35 | INFO | hw5.seq2seq | training loss: 3.9684\n",
            "2022-04-07 15:06:35 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "386d98f5292e4ccaa04caac2b56416b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 15:07:31 | INFO | hw5.seq2seq | example source: this is market share of apache web server one of the critical applications in webbased communications .\n",
            "2022-04-07 15:07:31 | INFO | hw5.seq2seq | example hypothesis: 這是一個廣告廣告的市場 , 廣告是網站上最重要的應用之一 。\n",
            "2022-04-07 15:07:31 | INFO | hw5.seq2seq | example reference: 這是網路伺服器軟體apache的市佔率網路伺服器是網絡交流最重要的應用之一\n",
            "2022-04-07 15:07:31 | INFO | hw5.seq2seq | validation loss:\t3.2142\n",
            "2022-04-07 15:07:31 | INFO | hw5.seq2seq | BLEU = 21.78 55.3/30.1/17.6/10.8 (BP = 0.918 ratio = 0.921 hyp_len = 102815 ref_len = 111605)\n",
            "2022-04-07 15:07:38 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint6.pt\n",
            "2022-04-07 15:07:45 | INFO | hw5.seq2seq | end of epoch 6\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c96168edde64c9999296c84b6d8f134",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 7:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 16:08:37 | INFO | hw5.seq2seq | training loss: 3.8951\n",
            "2022-04-07 16:08:37 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d896dc0964a2409e99600c37842ab9af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 16:09:38 | INFO | hw5.seq2seq | example source: you think of them as flawed or defective .\n",
            "2022-04-07 16:09:38 | INFO | hw5.seq2seq | example hypothesis: 你把它們想成是平的、有缺陷的、有缺陷的 。\n",
            "2022-04-07 16:09:38 | INFO | hw5.seq2seq | example reference: 你認為他們有瑕疵或缺陷 。\n",
            "2022-04-07 16:09:38 | INFO | hw5.seq2seq | validation loss:\t3.1547\n",
            "2022-04-07 16:09:38 | INFO | hw5.seq2seq | BLEU = 23.06 53.6/29.4/17.1/10.6 (BP = 0.998 ratio = 0.998 hyp_len = 111333 ref_len = 111605)\n",
            "2022-04-07 16:09:45 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint7.pt\n",
            "2022-04-07 16:09:52 | INFO | hw5.seq2seq | end of epoch 7\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c8d05d6615b47a49217ef91b23070d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 8:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 17:10:41 | INFO | hw5.seq2seq | training loss: 3.8358\n",
            "2022-04-07 17:10:41 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ce2cb6279f44663be4dd6bbe3ac8104",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 17:11:40 | INFO | hw5.seq2seq | example source: outside of nollywood , the image of africa remains frozen in the old \" national geographic \" mode and safari perspective .\n",
            "2022-04-07 17:11:40 | INFO | hw5.seq2seq | example hypothesis: 在奈萊塢之外 , 非洲的影像在古老的 「 國家地理雜誌 」 及 「 撒利姆觀點 」 之外 。\n",
            "2022-04-07 17:11:40 | INFO | hw5.seq2seq | example reference: 在奈萊塢之外 , 非洲的形象仍然停留在老式的 「 國家地理頻道 」 模式以及狩獵角度 。\n",
            "2022-04-07 17:11:40 | INFO | hw5.seq2seq | validation loss:\t3.1096\n",
            "2022-04-07 17:11:40 | INFO | hw5.seq2seq | BLEU = 23.39 55.4/30.8/18.3/11.5 (BP = 0.955 ratio = 0.956 hyp_len = 106674 ref_len = 111605)\n",
            "2022-04-07 17:11:46 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint8.pt\n",
            "2022-04-07 17:11:53 | INFO | hw5.seq2seq | end of epoch 8\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "047ae7c132944b81b26a870befdf37ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 9:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 18:12:41 | INFO | hw5.seq2seq | training loss: 3.7861\n",
            "2022-04-07 18:12:41 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "acaacfcd4f5643ceb4b49ec8aa1d7c8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 18:13:41 | INFO | hw5.seq2seq | example source: samantha's voice is like a concentrated sample of red food dye which we can infuse into the recordings of her surrogate to get a pink voice just like this .\n",
            "2022-04-07 18:13:41 | INFO | hw5.seq2seq | example hypothesis: 薩曼莎的聲音就像是紅色食物的樣本 , 可以被用在她的外祖母的唱片中 , 來取得像這樣的粉紅聲音 。\n",
            "2022-04-07 18:13:41 | INFO | hw5.seq2seq | example reference: 珊曼莎的聲音就像是紅色食用色素的濃縮樣品我們可以將它注入到她代替者的錄音裡然後取得一個像這樣的粉色聲音\n",
            "2022-04-07 18:13:41 | INFO | hw5.seq2seq | validation loss:\t3.0761\n",
            "2022-04-07 18:13:41 | INFO | hw5.seq2seq | BLEU = 23.79 55.7/31.1/18.6/11.7 (BP = 0.960 ratio = 0.960 hyp_len = 107176 ref_len = 111605)\n",
            "2022-04-07 18:13:48 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint9.pt\n",
            "2022-04-07 18:13:54 | INFO | hw5.seq2seq | end of epoch 9\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86d79fd77b39451096a3f6ee591466d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 10:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f9b708831f0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/weiweichi/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/weiweichi/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f9b708831f0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/weiweichi/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/weiweichi/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 19:14:46 | INFO | hw5.seq2seq | training loss: 3.7427\n",
            "2022-04-07 19:14:46 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b398f37fe7cd4ad48d247930d90a005f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 19:15:48 | INFO | hw5.seq2seq | example source: i was just happy it wasn't a c or a d .\n",
            "2022-04-07 19:15:48 | INFO | hw5.seq2seq | example hypothesis: 我很高興它不是c或d 。\n",
            "2022-04-07 19:15:48 | INFO | hw5.seq2seq | example reference: 我很高興得到的不是c-或d 。\n",
            "2022-04-07 19:15:48 | INFO | hw5.seq2seq | validation loss:\t3.0487\n",
            "2022-04-07 19:15:48 | INFO | hw5.seq2seq | BLEU = 24.68 55.3/31.1/18.7/11.9 (BP = 0.993 ratio = 0.993 hyp_len = 110778 ref_len = 111605)\n",
            "2022-04-07 19:15:52 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint10.pt\n",
            "2022-04-07 19:15:57 | INFO | hw5.seq2seq | end of epoch 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fde82375c8854ce4bef8968b9c5b1885",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 11:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 20:16:47 | INFO | hw5.seq2seq | training loss: 3.7032\n",
            "2022-04-07 20:16:47 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f062fd7bbf142d58fa942bbf2f1e251",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 20:17:47 | INFO | hw5.seq2seq | example source: if the matter is certain , to whom is it so ? to calvin ?\n",
            "2022-04-07 20:17:47 | INFO | hw5.seq2seq | example hypothesis: 如果事情是肯定的 , 對於誰是肯定的呢 ? 對於卡文呢 ?\n",
            "2022-04-07 20:17:47 | INFO | hw5.seq2seq | example reference: 如果事情是確定的 , 對於誰而言的 ? 對於凱文 ?\n",
            "2022-04-07 20:17:47 | INFO | hw5.seq2seq | validation loss:\t3.0286\n",
            "2022-04-07 20:17:47 | INFO | hw5.seq2seq | BLEU = 24.64 56.0/31.5/19.0/12.1 (BP = 0.977 ratio = 0.977 hyp_len = 109014 ref_len = 111605)\n",
            "2022-04-07 20:17:51 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint11.pt\n",
            "2022-04-07 20:17:52 | INFO | hw5.seq2seq | end of epoch 11\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "339c202cbaec4be7a7cc7be0dcd6de8c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 12:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 21:18:42 | INFO | hw5.seq2seq | training loss: 3.6695\n",
            "2022-04-07 21:18:42 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9f6c72455714637bb0c04a0158985ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 21:19:41 | INFO | hw5.seq2seq | example source: our credit scores have helped us deliver over 200 , 000 loans in kenya in just the past year .\n",
            "2022-04-07 21:19:41 | INFO | hw5.seq2seq | example hypothesis: 我們的信用評估顯示 , 過去一年 , 在肯亞 , 肯亞有超過二十萬個貸款 。\n",
            "2022-04-07 21:19:41 | INFO | hw5.seq2seq | example reference: 我們的信用積分 , 已幫我們在肯亞釋放出200 , 000份貸款──僅僅在去年 。\n",
            "2022-04-07 21:19:41 | INFO | hw5.seq2seq | validation loss:\t3.0064\n",
            "2022-04-07 21:19:41 | INFO | hw5.seq2seq | BLEU = 24.50 56.7/32.0/19.3/12.3 (BP = 0.956 ratio = 0.956 hyp_len = 106747 ref_len = 111605)\n",
            "2022-04-07 21:19:46 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint12.pt\n",
            "2022-04-07 21:19:46 | INFO | hw5.seq2seq | end of epoch 12\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d49f1a0b33e64c1f8887f172a21a9767",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 13:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 22:20:35 | INFO | hw5.seq2seq | training loss: 3.6398\n",
            "2022-04-07 22:20:35 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6440fced858542d4a2408372478111e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 22:21:33 | INFO | hw5.seq2seq | example source: first , the chef said the number was greater than 500 but lied , meaning it’s actually less than 500 .\n",
            "2022-04-07 22:21:33 | INFO | hw5.seq2seq | example hypothesis: 首先 , 廚師說這個數字比500多人還大 , 但說實在的意思是 , 這數字比500多人還小 。\n",
            "2022-04-07 22:21:33 | INFO | hw5.seq2seq | example reference: 首先 , 名廚撒謊說數字大於500代表數字實際上小於500\n",
            "2022-04-07 22:21:33 | INFO | hw5.seq2seq | validation loss:\t2.9838\n",
            "2022-04-07 22:21:33 | INFO | hw5.seq2seq | BLEU = 24.89 56.8/32.3/19.6/12.5 (BP = 0.960 ratio = 0.961 hyp_len = 107268 ref_len = 111605)\n",
            "2022-04-07 22:21:37 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint13.pt\n",
            "2022-04-07 22:21:42 | INFO | hw5.seq2seq | end of epoch 13\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3a62d422524413e9309e5664096c399",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 14:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 23:22:32 | INFO | hw5.seq2seq | training loss: 3.6091\n",
            "2022-04-07 23:22:32 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2417f023a7b840c7bcc03c723e7e54a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-07 23:23:32 | INFO | hw5.seq2seq | example source: this has undermined the legacy and the impact of these programs , but it also makes them ripe for reclamation .\n",
            "2022-04-07 23:23:32 | INFO | hw5.seq2seq | example hypothesis: 這已經破壞了這些計畫的遺傳性和影響 , 但也讓它們變得有復原力 。\n",
            "2022-04-07 23:23:32 | INFO | hw5.seq2seq | example reference: 此舉逐漸破壞這些計畫的名聲及影響力 , 但也使重新再造的時機成熟 。\n",
            "2022-04-07 23:23:32 | INFO | hw5.seq2seq | validation loss:\t2.9694\n",
            "2022-04-07 23:23:32 | INFO | hw5.seq2seq | BLEU = 25.20 56.6/32.1/19.4/12.4 (BP = 0.980 ratio = 0.980 hyp_len = 109356 ref_len = 111605)\n",
            "2022-04-07 23:23:36 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint14.pt\n",
            "2022-04-07 23:23:41 | INFO | hw5.seq2seq | end of epoch 14\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13dcd77eee08427a8b198de21550e3a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 15:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 00:24:31 | INFO | hw5.seq2seq | training loss: 3.5848\n",
            "2022-04-08 00:24:31 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9689855b27b9473489e4d25b5b2ea6d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 00:25:32 | INFO | hw5.seq2seq | example source: now it was not about riverside school .\n",
            "2022-04-08 00:25:32 | INFO | hw5.seq2seq | example hypothesis: 那不是關於河濱學校 。\n",
            "2022-04-08 00:25:32 | INFO | hw5.seq2seq | example reference: 這行動已經無關乎 「 河濱學校 」 本身\n",
            "2022-04-08 00:25:32 | INFO | hw5.seq2seq | validation loss:\t2.9517\n",
            "2022-04-08 00:25:32 | INFO | hw5.seq2seq | BLEU = 25.34 55.8/31.8/19.3/12.4 (BP = 0.993 ratio = 0.993 hyp_len = 110789 ref_len = 111605)\n",
            "2022-04-08 00:25:38 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint15.pt\n",
            "2022-04-08 00:25:45 | INFO | hw5.seq2seq | end of epoch 15\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "981e06eb076c46929a6af633bc368920",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 16:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 01:26:32 | INFO | hw5.seq2seq | training loss: 3.5591\n",
            "2022-04-08 01:26:32 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9cd4fa402371476d9b6b52998be74eaa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 01:27:31 | INFO | hw5.seq2seq | example source: this liwc software generated the frequencies of promotion and prevention words in the transcribed text .\n",
            "2022-04-08 01:27:31 | INFO | hw5.seq2seq | example hypothesis: 這個軟體產生了提倡和預防用字的頻率和預防用字 。\n",
            "2022-04-08 01:27:31 | INFO | hw5.seq2seq | example reference: liwc軟體能夠算出在文字記錄當中 , 促進式和預防式的用字出現了多少次 。\n",
            "2022-04-08 01:27:31 | INFO | hw5.seq2seq | validation loss:\t2.9394\n",
            "2022-04-08 01:27:31 | INFO | hw5.seq2seq | BLEU = 25.65 56.9/32.6/19.9/12.8 (BP = 0.978 ratio = 0.979 hyp_len = 109229 ref_len = 111605)\n",
            "2022-04-08 01:27:37 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint16.pt\n",
            "2022-04-08 01:27:44 | INFO | hw5.seq2seq | end of epoch 16\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf2889081fdc41cda459de098f934135",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 17:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 02:28:32 | INFO | hw5.seq2seq | training loss: 3.5372\n",
            "2022-04-08 02:28:32 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c6c2cd4fc364b219f36b67ebddea195",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 02:29:32 | INFO | hw5.seq2seq | example source: so i'm smiling with a little bit of embarrassment right now , because obviously , it's completely absurd that a single person , let alone a planner , could save a city .\n",
            "2022-04-08 02:29:32 | INFO | hw5.seq2seq | example hypothesis: 我現在在微笑 , 有點不好意思 , 因為很顯然 , 單獨一個人是完全荒謬的 , 更別說一個規劃者 , 可以拯救一個城市 。\n",
            "2022-04-08 02:29:32 | INFO | hw5.seq2seq | example reference: 我的笑裡帶著一絲尷尬 , 因為這很明顯 , 某一個人 , 一個規劃者 , 能去拯救一座城市是荒謬的 。\n",
            "2022-04-08 02:29:32 | INFO | hw5.seq2seq | validation loss:\t2.9283\n",
            "2022-04-08 02:29:32 | INFO | hw5.seq2seq | BLEU = 25.74 56.4/32.3/19.8/12.8 (BP = 0.989 ratio = 0.989 hyp_len = 110349 ref_len = 111605)\n",
            "2022-04-08 02:29:39 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint17.pt\n",
            "2022-04-08 02:29:45 | INFO | hw5.seq2seq | end of epoch 17\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f84ba8082450431e92402a27d4c8b957",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 18:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 03:30:34 | INFO | hw5.seq2seq | training loss: 3.5179\n",
            "2022-04-08 03:30:34 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db6c5a0236b2491cb858e44fa0456885",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 03:31:34 | INFO | hw5.seq2seq | example source: typically , it's a doctor giving a job to a nurse .\n",
            "2022-04-08 03:31:34 | INFO | hw5.seq2seq | example hypothesis: 通常 , 醫生會給護士一份工作 。\n",
            "2022-04-08 03:31:34 | INFO | hw5.seq2seq | example reference: 正常來講 , 是由醫生囑咐護士\n",
            "2022-04-08 03:31:34 | INFO | hw5.seq2seq | validation loss:\t2.9174\n",
            "2022-04-08 03:31:34 | INFO | hw5.seq2seq | BLEU = 25.89 56.3/32.3/19.8/12.8 (BP = 0.994 ratio = 0.994 hyp_len = 110882 ref_len = 111605)\n",
            "2022-04-08 03:31:41 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint18.pt\n",
            "2022-04-08 03:31:48 | INFO | hw5.seq2seq | end of epoch 18\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e38f56bbf814963bf774a7d3f16b6ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 19:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 04:32:36 | INFO | hw5.seq2seq | training loss: 3.4990\n",
            "2022-04-08 04:32:36 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c688ea323aa6408ea38c90f03a7329b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 04:33:34 | INFO | hw5.seq2seq | example source: it is going crazy with the arab spring and revolution and all this .\n",
            "2022-04-08 04:33:34 | INFO | hw5.seq2seq | example hypothesis: 阿拉伯之春、革命等 , 都很瘋狂 。\n",
            "2022-04-08 04:33:34 | INFO | hw5.seq2seq | example reference: 中東正為阿拉伯之春狂熱著 , 為革命以及所有這一切狂熱著 。 今晚在這兒有黎巴嫩人嗎 ?\n",
            "2022-04-08 04:33:34 | INFO | hw5.seq2seq | validation loss:\t2.9085\n",
            "2022-04-08 04:33:34 | INFO | hw5.seq2seq | BLEU = 25.87 58.1/33.5/20.7/13.6 (BP = 0.952 ratio = 0.953 hyp_len = 106332 ref_len = 111605)\n",
            "2022-04-08 04:33:40 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint19.pt\n",
            "2022-04-08 04:33:40 | INFO | hw5.seq2seq | end of epoch 19\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ef16a373a194fd68a7df64b0b9840e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 20:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 05:34:29 | INFO | hw5.seq2seq | training loss: 3.4806\n",
            "2022-04-08 05:34:29 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "633e5fc81b1c4a599e18e456c958c738",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 05:35:26 | INFO | hw5.seq2seq | example source: the selling of girls is rampant across the planet .\n",
            "2022-04-08 05:35:26 | INFO | hw5.seq2seq | example hypothesis: 女孩的賣淫率在全世界都很驚人 。\n",
            "2022-04-08 05:35:26 | INFO | hw5.seq2seq | example reference: 販賣女孩這種行為猖獗於整個世界\n",
            "2022-04-08 05:35:26 | INFO | hw5.seq2seq | validation loss:\t2.8991\n",
            "2022-04-08 05:35:26 | INFO | hw5.seq2seq | BLEU = 25.81 57.7/33.1/20.4/13.2 (BP = 0.965 ratio = 0.965 hyp_len = 107717 ref_len = 111605)\n",
            "2022-04-08 05:35:33 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint20.pt\n",
            "2022-04-08 05:35:33 | INFO | hw5.seq2seq | end of epoch 20\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46eee0f3765b4962b45e3f016c99d34b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 21:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 06:36:21 | INFO | hw5.seq2seq | training loss: 3.4636\n",
            "2022-04-08 06:36:21 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04e740999a5443eda408623f45769711",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 06:37:20 | INFO | hw5.seq2seq | example source: when the red army occupied east germany in 1945 , it immediately expanded there , and soon it started to train the german communists to build up their own secret police .\n",
            "2022-04-08 06:37:20 | INFO | hw5.seq2seq | example hypothesis: 當1945年德國佔領東德時 , 馬上擴大到那裡 , 很快地 , 德國共產黨開始訓練德國共產黨 , 建立他們自己的秘密警察 。\n",
            "2022-04-08 06:37:20 | INFO | hw5.seq2seq | example reference: 1945年 , 當蘇聯紅軍佔領東德時紅軍勢力立即擴展到那裡 , 很快地對德國共產黨展開訓練建立他們專屬的秘密警察 。\n",
            "2022-04-08 06:37:20 | INFO | hw5.seq2seq | validation loss:\t2.8942\n",
            "2022-04-08 06:37:20 | INFO | hw5.seq2seq | BLEU = 26.23 57.4/33.1/20.4/13.2 (BP = 0.980 ratio = 0.980 hyp_len = 109386 ref_len = 111605)\n",
            "2022-04-08 06:37:27 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint21.pt\n",
            "2022-04-08 06:37:33 | INFO | hw5.seq2seq | end of epoch 21\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9398d43acbc54056a6ffa9916a2d0cd0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 22:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 07:38:22 | INFO | hw5.seq2seq | training loss: 3.4462\n",
            "2022-04-08 07:38:22 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2293beaca29a4da689bd99d9f4478239",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 07:39:21 | INFO | hw5.seq2seq | example source: it became entertainment ; it became a new kind of commodity , something i was selling .\n",
            "2022-04-08 07:39:21 | INFO | hw5.seq2seq | example hypothesis: 它變成了娛樂 , 變成了一種新的商品 , 一種我銷售的產品 。\n",
            "2022-04-08 07:39:21 | INFO | hw5.seq2seq | example reference: 它變成了一個玩賞產物 , 一種新出品的貨物 , 一些我用來賣出去的東西 。\n",
            "2022-04-08 07:39:21 | INFO | hw5.seq2seq | validation loss:\t2.8876\n",
            "2022-04-08 07:39:21 | INFO | hw5.seq2seq | BLEU = 26.34 57.4/33.2/20.6/13.5 (BP = 0.976 ratio = 0.976 hyp_len = 108921 ref_len = 111605)\n",
            "2022-04-08 07:39:28 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint22.pt\n",
            "2022-04-08 07:39:35 | INFO | hw5.seq2seq | end of epoch 22\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46fa4b215ac24ccdb159dc3d35da8fb7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 23:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 08:40:23 | INFO | hw5.seq2seq | training loss: 3.4302\n",
            "2022-04-08 08:40:23 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02a8aab19fee483cbe85ea98ab64e004",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 08:41:24 | INFO | hw5.seq2seq | example source: and i think that's because they've been optimized to pass , as we always expect people to do , to optimize relative to brightline rules about how affected the car will be .\n",
            "2022-04-08 08:41:24 | INFO | hw5.seq2seq | example hypothesis: 我想這是因為他們一直以來都被最佳化過 , 正如我們總是期待人們可以做到的 , 最佳化到最佳化到最佳化到最佳化到最佳化到最佳化到最佳化到最佳狀態 。\n",
            "2022-04-08 08:41:24 | INFO | hw5.seq2seq | example reference: 我想這是因為他們都是針對車頭正面沖撞來設計的 。 就象大家經常會做的 , 上有政策 , 下有對策 , 在汽車安全這一問題上也是一樣 。\n",
            "2022-04-08 08:41:24 | INFO | hw5.seq2seq | validation loss:\t2.8799\n",
            "2022-04-08 08:41:24 | INFO | hw5.seq2seq | BLEU = 26.34 56.5/32.5/20.1/13.1 (BP = 1.000 ratio = 1.000 hyp_len = 111574 ref_len = 111605)\n",
            "2022-04-08 08:41:31 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint23.pt\n",
            "2022-04-08 08:41:37 | INFO | hw5.seq2seq | end of epoch 23\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5cbab4e0af64312860494202b13de1f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 24:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 09:42:29 | INFO | hw5.seq2seq | training loss: 3.4164\n",
            "2022-04-08 09:42:29 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c2a628f93ac43848fd910131566af8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 09:43:28 | INFO | hw5.seq2seq | example source: rodney mullen: that's a good question . kg: something tells me it's not the end .\n",
            "2022-04-08 09:43:28 | INFO | hw5.seq2seq | example hypothesis: rodneymullen:這是個好問題 。 kb:有些事情會讓我覺得不舒服 。\n",
            "2022-04-08 09:43:28 | INFO | hw5.seq2seq | example reference: rodneymullen:好問題 。 kh:直覺似乎未到終結 。\n",
            "2022-04-08 09:43:28 | INFO | hw5.seq2seq | validation loss:\t2.8719\n",
            "2022-04-08 09:43:28 | INFO | hw5.seq2seq | BLEU = 26.33 57.8/33.4/20.7/13.6 (BP = 0.971 ratio = 0.971 hyp_len = 108411 ref_len = 111605)\n",
            "2022-04-08 09:43:34 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint24.pt\n",
            "2022-04-08 09:43:34 | INFO | hw5.seq2seq | end of epoch 24\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b73d6b248bf45998af60bbbd2742786",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 25:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 10:44:21 | INFO | hw5.seq2seq | training loss: 3.4037\n",
            "2022-04-08 10:44:21 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20f8941eadf44c588f759c617f966406",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 10:45:21 | INFO | hw5.seq2seq | example source: i would have stopped to help . \"\n",
            "2022-04-08 10:45:21 | INFO | hw5.seq2seq | example hypothesis: 我會停下來幫忙 。 」\n",
            "2022-04-08 10:45:21 | INFO | hw5.seq2seq | example reference: 我會停下來救她 。 」\n",
            "2022-04-08 10:45:21 | INFO | hw5.seq2seq | validation loss:\t2.8679\n",
            "2022-04-08 10:45:21 | INFO | hw5.seq2seq | BLEU = 26.60 57.4/33.3/20.7/13.6 (BP = 0.983 ratio = 0.983 hyp_len = 109741 ref_len = 111605)\n",
            "2022-04-08 10:45:28 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint25.pt\n",
            "2022-04-08 10:45:34 | INFO | hw5.seq2seq | end of epoch 25\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e168650779fe457ba2f6c6492353b11b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 26:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 11:46:22 | INFO | hw5.seq2seq | training loss: 3.3913\n",
            "2022-04-08 11:46:22 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d53a93b8d2f4173a911cf8d4e5019c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 11:47:22 | INFO | hw5.seq2seq | example source: vulnerability pushed , i pushed back .\n",
            "2022-04-08 11:47:22 | INFO | hw5.seq2seq | example hypothesis: 脆弱感促使我向後推 ,\n",
            "2022-04-08 11:47:22 | INFO | hw5.seq2seq | example reference: 脆弱揍我 , 我打回去\n",
            "2022-04-08 11:47:22 | INFO | hw5.seq2seq | validation loss:\t2.8620\n",
            "2022-04-08 11:47:22 | INFO | hw5.seq2seq | BLEU = 26.41 57.1/33.0/20.5/13.4 (BP = 0.985 ratio = 0.985 hyp_len = 109900 ref_len = 111605)\n",
            "2022-04-08 11:47:29 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint26.pt\n",
            "2022-04-08 11:47:29 | INFO | hw5.seq2seq | end of epoch 26\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8697dfe97bd4819a61153763b4e19cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 27:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 12:48:17 | INFO | hw5.seq2seq | training loss: 3.3789\n",
            "2022-04-08 12:48:17 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "973eefc37cee42759bac592508f604e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 12:49:17 | INFO | hw5.seq2seq | example source: now i want you to imagine for a second an 18yearold boy who graduates from high school in kansas city , missouri .\n",
            "2022-04-08 12:49:17 | INFO | hw5.seq2seq | example hypothesis: 現在請各位想像一個十八歲的男孩他在密蘇里州堪薩斯城高中畢業\n",
            "2022-04-08 12:49:17 | INFO | hw5.seq2seq | example reference: 現在我要你們想像一個18歲的男孩 , 剛從密蘇里州堪薩斯城的高中畢業 。\n",
            "2022-04-08 12:49:17 | INFO | hw5.seq2seq | validation loss:\t2.8539\n",
            "2022-04-08 12:49:17 | INFO | hw5.seq2seq | BLEU = 26.34 56.4/32.6/20.2/13.3 (BP = 0.994 ratio = 0.994 hyp_len = 110967 ref_len = 111605)\n",
            "2022-04-08 12:49:24 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint27.pt\n",
            "2022-04-08 12:49:24 | INFO | hw5.seq2seq | end of epoch 27\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47e3d43453aa4ca99b4d4fbeb77ba3c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 28:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 13:50:10 | INFO | hw5.seq2seq | training loss: 3.3670\n",
            "2022-04-08 13:50:10 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d8388d10fa54313a62a1dc94f2252fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 13:51:11 | INFO | hw5.seq2seq | example source: some leave by crossing the sahara desert .\n",
            "2022-04-08 13:51:11 | INFO | hw5.seq2seq | example hypothesis: 有些人會離開 , 穿越撒哈拉沙漠 。\n",
            "2022-04-08 13:51:11 | INFO | hw5.seq2seq | example reference: 有些人是穿越撒哈拉沙漠離開 ,\n",
            "2022-04-08 13:51:11 | INFO | hw5.seq2seq | validation loss:\t2.8496\n",
            "2022-04-08 13:51:11 | INFO | hw5.seq2seq | BLEU = 26.75 57.0/33.0/20.7/13.7 (BP = 0.991 ratio = 0.991 hyp_len = 110571 ref_len = 111605)\n",
            "2022-04-08 13:51:17 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint28.pt\n",
            "2022-04-08 13:51:24 | INFO | hw5.seq2seq | end of epoch 28\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b795c0ae46c4344a2472791628427c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 29:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 14:52:12 | INFO | hw5.seq2seq | training loss: 3.3569\n",
            "2022-04-08 14:52:12 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c17b6cf05934bac8ae662b1a8992652",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 14:53:13 | INFO | hw5.seq2seq | example source: and we call them active galactic nuclei .\n",
            "2022-04-08 14:53:13 | INFO | hw5.seq2seq | example hypothesis: 我們稱他們為 「 銀河系核心 」 。\n",
            "2022-04-08 14:53:13 | INFO | hw5.seq2seq | example reference: 我们叫它们活跃星系核子 。\n",
            "2022-04-08 14:53:13 | INFO | hw5.seq2seq | validation loss:\t2.8465\n",
            "2022-04-08 14:53:13 | INFO | hw5.seq2seq | BLEU = 26.84 56.7/32.9/20.6/13.5 (BP = 1.000 ratio = 1.001 hyp_len = 111764 ref_len = 111605)\n",
            "2022-04-08 14:53:20 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint29.pt\n",
            "2022-04-08 14:53:27 | INFO | hw5.seq2seq | end of epoch 29\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28acc98f2248407fb73ad23901df22be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 30:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 15:54:16 | INFO | hw5.seq2seq | training loss: 3.3453\n",
            "2022-04-08 15:54:16 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57d87155b73e4a70aa8e9d789fa2714b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 15:55:15 | INFO | hw5.seq2seq | example source: i hack into your lives for a living .\n",
            "2022-04-08 15:55:15 | INFO | hw5.seq2seq | example hypothesis: 我入侵了你的生活 , 為了生存 。\n",
            "2022-04-08 15:55:15 | INFO | hw5.seq2seq | example reference: 我把你們的生活都研究透徹了\n",
            "2022-04-08 15:55:15 | INFO | hw5.seq2seq | validation loss:\t2.8434\n",
            "2022-04-08 15:55:15 | INFO | hw5.seq2seq | BLEU = 26.59 57.4/33.3/20.7/13.7 (BP = 0.981 ratio = 0.981 hyp_len = 109461 ref_len = 111605)\n",
            "2022-04-08 15:55:22 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint30.pt\n",
            "2022-04-08 15:55:22 | INFO | hw5.seq2seq | end of epoch 30\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d082e32312f042c2a4ee98243622a3ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 31:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 16:56:09 | INFO | hw5.seq2seq | training loss: 3.3362\n",
            "2022-04-08 16:56:09 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "769c7157a73c4de0b398eea80b96c97b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 16:57:07 | INFO | hw5.seq2seq | example source: we were successful in namibia because we dreamed of a future that was much more than just a healthy wildlife .\n",
            "2022-04-08 16:57:07 | INFO | hw5.seq2seq | example hypothesis: 我們在納米比亞成功了 , 因為我們夢想的未來不只是一個健康的野生動物 。\n",
            "2022-04-08 16:57:07 | INFO | hw5.seq2seq | example reference: 我們在納米比亞的成功是因為我們夢想我們的未來它會比一個健康的野生環境還要更遠大\n",
            "2022-04-08 16:57:07 | INFO | hw5.seq2seq | validation loss:\t2.8373\n",
            "2022-04-08 16:57:07 | INFO | hw5.seq2seq | BLEU = 26.49 57.7/33.5/20.8/13.7 (BP = 0.972 ratio = 0.972 hyp_len = 108482 ref_len = 111605)\n",
            "2022-04-08 16:57:14 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint31.pt\n",
            "2022-04-08 16:57:14 | INFO | hw5.seq2seq | end of epoch 31\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5148e95d44aa4fdc97179499c4bfad8a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 32:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 17:58:01 | INFO | hw5.seq2seq | training loss: 3.3242\n",
            "2022-04-08 17:58:01 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc5288b6a1d14a3faf7266bf79c45ac2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 17:59:00 | INFO | hw5.seq2seq | example source: it's not so important what specific degree upstreamists have at the end of their name .\n",
            "2022-04-08 17:59:00 | INFO | hw5.seq2seq | example hypothesis: 並不是那麼重要 , 特定的上游管理人在名單的末端 。\n",
            "2022-04-08 17:59:00 | INFO | hw5.seq2seq | example reference: 上游管理人在名字後面掛什麼學位頭銜並不重要 。\n",
            "2022-04-08 17:59:00 | INFO | hw5.seq2seq | validation loss:\t2.8317\n",
            "2022-04-08 17:59:00 | INFO | hw5.seq2seq | BLEU = 26.58 57.7/33.6/21.0/13.9 (BP = 0.968 ratio = 0.969 hyp_len = 108137 ref_len = 111605)\n",
            "2022-04-08 17:59:07 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint32.pt\n",
            "2022-04-08 17:59:07 | INFO | hw5.seq2seq | end of epoch 32\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14f438a4f84445edbc2ca51c4e58702f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 33:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 18:59:53 | INFO | hw5.seq2seq | training loss: 3.3146\n",
            "2022-04-08 18:59:53 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "442b55194dde45fe8592f612a8dc2cf7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 19:00:53 | INFO | hw5.seq2seq | example source: i made a physical version of flappy bird that could never be taken off the app store .\n",
            "2022-04-08 19:00:53 | INFO | hw5.seq2seq | example hypothesis: 我做了一個實體版本的 「 笨鳥先飛 」 , 可能永遠不會被拿走 。\n",
            "2022-04-08 19:00:53 | INFO | hw5.seq2seq | example reference: 我製作了實體版的 「 笨鳥先飛 」 , 而它不會從應用程式商店下架 。\n",
            "2022-04-08 19:00:53 | INFO | hw5.seq2seq | validation loss:\t2.8301\n",
            "2022-04-08 19:00:53 | INFO | hw5.seq2seq | BLEU = 26.73 57.3/33.3/20.8/13.7 (BP = 0.984 ratio = 0.984 hyp_len = 109819 ref_len = 111605)\n",
            "2022-04-08 19:01:00 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint33.pt\n",
            "2022-04-08 19:01:00 | INFO | hw5.seq2seq | end of epoch 33\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6ea0e08e3174b88b0a4ab3d840093b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 34:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 20:01:47 | INFO | hw5.seq2seq | training loss: 3.3059\n",
            "2022-04-08 20:01:47 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8975554d11614414881f82de52f578cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 20:02:48 | INFO | hw5.seq2seq | example source: so therefore the new requirement is repairability , making cars easy to repair .\n",
            "2022-04-08 20:02:48 | INFO | hw5.seq2seq | example hypothesis: 因此 , 新的條件就是可維修性 , 讓汽車更容易維修 。\n",
            "2022-04-08 20:02:48 | INFO | hw5.seq2seq | example reference: 那麼這項新的條件就是可維修性 , 讓維修汽車變得容易 ,\n",
            "2022-04-08 20:02:48 | INFO | hw5.seq2seq | validation loss:\t2.8303\n",
            "2022-04-08 20:02:48 | INFO | hw5.seq2seq | BLEU = 26.69 56.9/33.0/20.6/13.6 (BP = 0.991 ratio = 0.991 hyp_len = 110572 ref_len = 111605)\n",
            "2022-04-08 20:02:55 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint34.pt\n",
            "2022-04-08 20:02:55 | INFO | hw5.seq2seq | end of epoch 34\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f6a22645ee241c5bf5a2c5273b3e2c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train epoch 35:   0%|          | 0/2178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 21:03:42 | INFO | hw5.seq2seq | training loss: 3.2969\n",
            "2022-04-08 21:03:42 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9281722910da43c685a9931eb38f128e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 21:04:41 | INFO | hw5.seq2seq | example source: there is tragically nothing that can be done to save those in the fireball’s radius .\n",
            "2022-04-08 21:04:41 | INFO | hw5.seq2seq | example hypothesis: 不幸的是 , 沒辦法拯救火球半徑的人 。\n",
            "2022-04-08 21:04:41 | INFO | hw5.seq2seq | example reference: 很遺憾的是 , 沒有任何辦法可以拯救火球半徑內的人 。\n",
            "2022-04-08 21:04:41 | INFO | hw5.seq2seq | validation loss:\t2.8321\n",
            "2022-04-08 21:04:41 | INFO | hw5.seq2seq | BLEU = 26.75 57.4/33.4/20.8/13.7 (BP = 0.982 ratio = 0.983 hyp_len = 109659 ref_len = 111605)\n",
            "2022-04-08 21:04:48 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/weiweichi/weichi/HW5/checkpoints/transformer/checkpoint35.pt\n",
            "2022-04-08 21:04:48 | INFO | hw5.seq2seq | end of epoch 35\n"
          ]
        }
      ],
      "source": [
        "epoch_itr = load_data_iterator(task, \"train\", config.start_epoch, config.max_tokens, config.num_workers)\n",
        "try_load_checkpoint(model, optimizer, name=config.resume)\n",
        "while epoch_itr.next_epoch_idx <= config.max_epoch:\n",
        "    # train for one epoch\n",
        "    train_one_epoch(epoch_itr, model, task, criterion, optimizer, config.accum_steps)\n",
        "    stats = validate_and_save(model, task, criterion, optimizer, epoch=epoch_itr.epoch)\n",
        "    logger.info(\"end of epoch {}\".format(epoch_itr.epoch))    \n",
        "    epoch_itr = load_data_iterator(task, \"train\", epoch_itr.next_epoch_idx, config.max_tokens, config.num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(checkpoint_upper_bound=None, inputs=['./checkpoints/transformer'], num_epoch_checkpoints=5, num_update_checkpoints=None, output='./checkpoints/transformer/avg_last_5_checkpoint.pt')\n",
            "averaging checkpoints:  ['./checkpoints/transformer/checkpoint35.pt', './checkpoints/transformer/checkpoint34.pt', './checkpoints/transformer/checkpoint33.pt', './checkpoints/transformer/checkpoint32.pt', './checkpoints/transformer/checkpoint31.pt']\n",
            "Finished writing averaged checkpoint to ./checkpoints/transformer/avg_last_5_checkpoint.pt\n"
          ]
        }
      ],
      "source": [
        "# averaging a few checkpoints can have a similar effect to ensemble\n",
        "checkdir=config.savedir\n",
        "!python3 ./fairseq/scripts/average_checkpoints.py \\\n",
        "--inputs {checkdir} \\\n",
        "--num-epoch-checkpoints 5 \\\n",
        "--output {checkdir}/avg_last_5_checkpoint.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 21:04:53 | INFO | hw5.seq2seq | loaded checkpoint checkpoints/transformer/avg_last_5_checkpoint.pt: step=unknown loss=2.832104444503784 bleu=26.745837225170764\n",
            "2022-04-08 21:04:53 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34dc1097116045058e23bd37ca831a06",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 21:05:52 | INFO | hw5.seq2seq | example source: now , they don't know how ridiculous that is , but i do .\n",
            "2022-04-08 21:05:52 | INFO | hw5.seq2seq | example hypothesis: 他們不知道這有多荒謬 , 但我知道 。\n",
            "2022-04-08 21:05:52 | INFO | hw5.seq2seq | example reference: 他們不知道那有多荒謬 , 而我知道 。\n",
            "2022-04-08 21:05:52 | INFO | hw5.seq2seq | validation loss:\t2.8098\n",
            "2022-04-08 21:05:52 | INFO | hw5.seq2seq | BLEU = 26.90 57.9/33.8/21.1/13.9 (BP = 0.978 ratio = 0.978 hyp_len = 109173 ref_len = 111605)\n"
          ]
        }
      ],
      "source": [
        "try_load_checkpoint(model, name=\"avg_last_5_checkpoint.pt\")\n",
        "validate(model, task, criterion, log_to_wandb=False)\n",
        "None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-08 21:05:52 | INFO | fairseq.data.data_utils | loaded 4,000 examples from: ./DATA/data-bin/ted2020_with_mono/test.en-zh.en\n",
            "2022-04-08 21:05:52 | INFO | fairseq.data.data_utils | loaded 4,000 examples from: ./DATA/data-bin/ted2020_with_mono/test.en-zh.zh\n",
            "2022-04-08 21:05:52 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020_with_mono test en-zh 4000 examples\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45aeda1a5eff416ba1159c28a9f0df72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "prediction:   0%|          | 0/42 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "generate_prediction(model, task, outfile=\"./prediction_bt.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Report\n",
        "* Problem1: Visualize the similarity between different pairs of positional embedding and briefly explain the result.\n",
        "\n",
        "* Problem2: Clip gradient norm and visualize the changes of gradient norm in different steps. Circle two places with gradient explosion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'similarity_matrix')"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "pos_emb = model.decoder.embed_positions.weights.cpu().detach()\n",
        "cos = nn.CosineSimilarity(dim=0)\n",
        "n = pos_emb.size(0)\n",
        "similarity_m = np.zeros((n, n))\n",
        "\n",
        "for i in range(n):\n",
        "    for j in range(i, n):\n",
        "        cs = cos(pos_emb[i], pos_emb[j])\n",
        "        similarity_m[i][j] = similarity_m[j][i] = cs\n",
        "sns.heatmap(similarity_m, xticklabels=100, yticklabels=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "# use wandb to record gnorm\n",
        "# https://wandb.ai/weiweichi/hw5.seq2seq/reports/Shared-panel-22-04-06-03-04-04--VmlldzoxNzk0MzQ0?accessToken=mwlfuganu5gl0bk4saqkru69kxvt5wxac2gymbqqxkzd4uwjij56sj1y8qmk6rlx"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "nKb4u67-sT_Z",
        "n1rwQysTsdJq",
        "59si_C0Wsms7",
        "oOpG4EBRLwe_",
        "6ZlE_1JnMv56",
        "UDAPmxjRNEEL",
        "ce5n4eS7NQNy",
        "rUB9f1WCNgMH",
        "VFJlkOMONsc6",
        "Gt1lX3DRO_yU",
        "BAGMiun8PnZy",
        "JOVQRHzGQU4-",
        "jegH0bvMQVmR",
        "a65glBVXQZiE",
        "smA0JraEQdxz",
        "Jn4XeawpQjLk"
      ],
      "name": "HW05.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
