

### pre-trained model:
* type1: [chinese_pretrain_mrc_macbert_large](https://huggingface.co/luhua/chinese_pretrain_mrc_macbert_large)
* type2: [hfl/chinese-pert-large](https://github.com/ymcui/PERT)
* type3: [roberta-base-chinese-extractive-qa](https://huggingface.co/uer/roberta-base-chinese-extractive-qa)

